{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyuns6100/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import Counter\n",
    "import collections\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from torchtext.vocab import Vocab, Vectors\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>> CUDA available : True\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "print(\">>>>>>>>>>> CUDA available :\", torch.cuda.is_available())\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GPU 사용 가능 -> True, GPU 사용 불가 -> False\n",
    "# print(torch.cuda.is_available())\n",
    "# tensor = torch.rand(3, 3).cuda()\n",
    "# print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'data_path' : \"/home/hyuns6100/Mental-Heatlh-Care/data/dailydialog_conv35seq_splits.json\", # 데이터 경로\n",
    "    'result_path': \"/home/hyuns6100/Mental-Heatlh-Care/Result/\",\n",
    "    'result_text_path': \"/home/hyuns6100/Mental-Heatlh-Care/Result/result_text/\",\n",
    "    'best_result_path': \"/home/hyuns6100/Mental-Heatlh-Care/Result/best_results.pkl\",\n",
    "    'wv_path': \"/home/hyuns6100/Mental-Heatlh-Care/data/\", \n",
    "    'word_vector':\"wiki-news-300d-1M.vec\",\n",
    "    \n",
    "    'lr': 1e-3,\n",
    "    'batch_size':32,\n",
    "    'train_epochs':1000,\n",
    "    'n_classes': 7,\n",
    "    'n_train_class': 7, \n",
    "\t'n_val_class': 7, \n",
    "\t'n_test_class': 7, \n",
    "    'labels': [1, 2, 3, 4, 5, 6],\n",
    " \n",
    "    'cnn_num_filters': 100,\n",
    "    'cnn_filter_sizes': [3,4,5],\n",
    "    'context_size': 35,\n",
    "    'maxtokens': 30,\n",
    "    'mlp_hidden': [300,300],\n",
    "    'dropout': 0.1,\n",
    "    \n",
    "    'seed':330,\n",
    "    'patience_metric': 'f1_micro',\n",
    "    'finetune_ebd': False,\n",
    "    'patience': 30,\n",
    "    'save': True,\n",
    "    'authors':False,\n",
    "    'convmode': 'seq',\n",
    "    'embedding': 'cnn',\n",
    "    'classifier': 'mlp'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "        Setting random seeds\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 토큰화 (이미 처리됨)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def creaDailyDialogSeq():\n",
    "#     \tprint(colored('CREATING DAILYDIALOG UNIFIED PREPROCESSED FILE FROM data/ijcnlp_dailydialog/  first make sure the per split dailydialog json files are there. Otherwise, please download dailydialog and run the formatting script as follows:', 'yellow'))\n",
    "# \tprint(\"\"\"cd data/ijcnlp_dailydialog\n",
    "# \tpython3 parser_gg.py -i data/ijcnlp_dailydialog/train -o data/train \n",
    "# \tpython3 parser_gg.py -i data/ijcnlp_dailydialog/validation -o data/validation\n",
    "# \tpython3 parser_gg.py -i data/ijcnlp_dailydialog/test -o data/test\"\"\")\n",
    "# \tsplits = {'train':'data/ijcnlp_dailydialog/train/dailydialog_train.json', 'test':'data/ijcnlp_dailydialog/test/dailydialog_test.json', 'val': 'data/ijcnlp_dailydialog/validation/dailydialog_validation.json'}\n",
    "# \tsplits_emotionflows_fp = { k: os.path.join( os.path.dirname(v), 'dailydialog_{}_emotionflow.json'.format(k) ) for k,v in splits.items() }\n",
    "# \ttxt2l = { 'no emotion': 0, 'anger': 1, 'disgust': 2, 'fear': 3, 'happiness': 4, 'sadness': 5, 'surprise': 6 }\n",
    "# \tl2txt = { v:k for k, v in txt2l.items() } \n",
    "# \temotionSet = list(set(list(txt2l.keys())))\n",
    "\n",
    "# \temotionFlows = {'train':[],'test':[],'val':[]}\n",
    "# \tdef getEmotionFlows(row, split):\n",
    "# \t\trow = json.loads(row)\n",
    "# \t\temotionFlow = [ l2txt[r['label']] for r in row ]\n",
    "# \t\temotionFlows[split].append(emotionFlow)\n",
    "\n",
    "# \tfor k,v in tqdm(splits.items()):\n",
    "# \t\tfor row in tqdm( open(v, 'r').read().split('\\n'), desc=colored(v, 'cyan') ) :\n",
    "# \t\t\tgetEmotionFlows(row, k)\n",
    "\n",
    "# \tseq_lengths = np.array([ len(x) for x in emotionFlows['train'] ] )\n",
    "# \tseq_size = np.percentile( seq_lengths, 90)\n",
    "# \t# seq_size = max(seq_lengths)\n",
    "# \tprint(colored('seq_size', 'yellow'), seq_size, '75%:', np.percentile(seq_lengths, 75), 'max:', max(seq_lengths))\n",
    "\n",
    "# \tdef tokenize_seq(chat):\n",
    "# \t\ttokenizer = TweetTokenizer()\n",
    "# \t\ttext =  [ ' '.join( list( map(lambda x: x.lower(), tokenizer.tokenize(m)) ) ) for m in chat['text'] ]\n",
    "# \t\tchat['text'] = text\n",
    "# \t\treturn chat\n",
    "\n",
    "# \tdef _trimpad(size, row, pad=True, trim=True):\n",
    "# \t\t''' trim and padding (with <pad>) '''\n",
    "# \t\tif len(row['texts']) > size:\n",
    "# \t\t\trow['texts'] = row['texts'][int(-size):]\n",
    "# \t\t\trow['labels'] = row['labels'][int(-size):]\n",
    "# \t\telse:\n",
    "# \t\t\trow['texts'] = [ ['<pad>' for j in range(5)] for k in range( int(size) - len(row['texts'])  ) ]   +   row['texts']\n",
    "# \t\t\trow['labels'] = [ 0 for k in range( int(size) - len(row['labels'])  ) ]   +   row['labels']\n",
    "\t\t\n",
    "# \t\tassert len(row['texts']) == size\n",
    "# \t\tassert len(row['labels']) == size\n",
    "# \t\treturn row\n",
    "\n",
    "# \tdataSplits = {'train':[], 'test': [], 'val':[]}\n",
    "# \tfor split in dataSplits.keys():\n",
    "# \t\tfor i, data in tqdm( enumerate( [ json.loads(line) for line in open(splits[split], 'r').read().split('\\n') ]  ), desc=colored('formatting sequence '+split, 'cyan'), total=len(emotionFlows[split]) ):\n",
    "# \t\t\t# data = [ tokenize_seq(chat) for chat in tqdm(data, total=len(data), desc='tokenizing') ]\n",
    "# \t\t\tdata = [ tokenize_seq(chat) for chat in data ]\n",
    "# \t\t\tentry = {'texts': [ x['text'] for x in data], 'labels': [ x['label'] for x in data], 'split': split}\n",
    "# \t\t\tentry = _trimpad(seq_size, entry)\n",
    "# \t\t\tdataSplits[split].append(entry)\n",
    "\t\n",
    "\t\n",
    "# \tlabels_train = Counter([ label for line in dataSplits['train'] for label in line['labels'] ])\n",
    "# \tlabels_val = Counter([ label for line in dataSplits['val'] for label in line['labels'] ])\n",
    "# \tlabels_test = Counter([ label for line in dataSplits['test'] for label in line['labels'] ])\n",
    "# \tprint(colored('labels balance', 'yellow'), labels_train, len(labels_train.keys()), labels_val, len(labels_val.keys()), labels_test, len(labels_test.keys()))\n",
    "\n",
    "# \trecords = dataSplits['train'] + dataSplits['val'] + dataSplits['test']\n",
    "# \tjsonLines = [json.dumps(line) for line  in tqdm(records)]\n",
    "# \twith open('data/dailydialog_conv{}seq_splits.json'.format(str(int(seq_size))), 'w') as f: f.write('\\n'.join(jsonLines))\n",
    "# \tprint( colored('data/dailydialog_conv%sseq_splits.json created!' % (str(int(seq_size))), 'green') )\n",
    "# \tprint(colored('You can now run the labelling tasks.', 'green'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dailydialog_seq_classes():\n",
    "    '''\n",
    "        seq consider no emotion label\n",
    "        @return list of classes associated with each split\n",
    "    '''\n",
    "    label_dict = { \n",
    "        'no emotion': 0, \n",
    "        'anger': 1, \n",
    "        'disgust': 2,\n",
    "        'fear': 3,\n",
    "        'happiness': 4,\n",
    "        'sadness': 5,\n",
    "        'surprise': 6\n",
    "    }\n",
    "\n",
    "    train_classes = [0, 1, 2, 3, 4, 5, 6]\n",
    "    val_classes = [0, 1, 2, 3, 4, 5, 6]\n",
    "    test_classes = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "    return train_classes, val_classes, test_classes\n",
    "\n",
    "def _load_json_seq(path, args):\n",
    "    '''\n",
    "        load data file\n",
    "        @param path: str, path to the data file\n",
    "        @return data: list of examples\n",
    "    '''\n",
    "    label = {}\n",
    "    text_len = []\n",
    "    with open(path, 'r', errors='ignore') as f:\n",
    "        data = []\n",
    "        for i, line in enumerate(f):\n",
    "            row = json.loads(line)\n",
    "\n",
    "            # count the number of examples per label\n",
    "            for l in row['labels']:\n",
    "                if int(l) not in label: label[int(l)] = 1\n",
    "                else: label[int(l)] += 1\n",
    "\n",
    "            item = {\n",
    "                'id': i+1,\n",
    "                'label': [int(r) for r in row['labels'] ],\n",
    "                # 'text': [ r[:args['maxtokens']] for r in row['texts'] ]  # 30 # 50 # 80 truncate the text to 500 tokens\n",
    "                'text': [ r[-args['maxtokens']:] for r in row['texts'] ]  # 30 # 50 # 80 truncate the text to the last tokens\n",
    "            }\n",
    "\n",
    "            if args.authors:\n",
    "                item.update({'authors': [ int(a) for a in row['authors'] ]})\n",
    "\n",
    "            if 'split' in row: item['split'] = row['split']\n",
    "\n",
    "            text_len.append(len(row['texts']))\n",
    "\n",
    "            data.append(item)\n",
    "\n",
    "        #tprint('Class balance (load_json_seq):')\n",
    "\n",
    "        print(label)\n",
    "\n",
    "        print('Avg len: {}'.format(sum(text_len) / (len(text_len))))\n",
    "        print('Max len: {}'.format(max(text_len)))\n",
    "\n",
    "        return data\n",
    "    \n",
    "def _read_words(data, convmode=None):\n",
    "    '''\n",
    "        Count the occurrences of all words\n",
    "        @param convmode: str, None for non conversational scope, 'naive' for classic or naive approach, 'conv' for conversation depth into account (one additional dim and nested values)\n",
    "        @param data: list of examples\n",
    "        @return words: list of words (with duplicates)\n",
    "    '''\n",
    "    words = []\n",
    "    if convmode is None:\n",
    "        for example in data:\n",
    "            words += example['text']\n",
    "    else:\n",
    "        for example in data:\n",
    "            for m in example['text']: \n",
    "                words += m     \n",
    "    \n",
    "    return words\n",
    "\n",
    "def _meta_split_by_field(all_data, train_classes, val_classes, test_classes, seqmode=False):\n",
    "    '''\n",
    "        Split the dataset according to the specified train_classes, val_classes\n",
    "        and test_classes\n",
    "        Consider a 'split' field for the different train test val sets\n",
    "\n",
    "        seqmode is a special mode to ensure sequences of labels to be taken into account\n",
    "\n",
    "        @param all_data: list of examples (dictionaries)\n",
    "        @param train_classes: list of int\n",
    "        @param val_classes: list of int\n",
    "        @param test_classes: list of int\n",
    "        @param seqmode: bool \n",
    "\n",
    "        @return train_data: list of examples\n",
    "        @return val_data: list of examples\n",
    "        @return test_data: list of examples\n",
    "    '''\n",
    "    train_data, val_data, test_data = [], [], []\n",
    "\n",
    "    if seqmode:\n",
    "        for example in all_data:\n",
    "            if example['split'] == 'train' and len(set(example['label']) & set(train_classes)) > 0: train_data.append(example)\n",
    "            if example['split'] == 'val' and len(set(example['label']) & set(val_classes)) > 0: val_data.append(example)\n",
    "            if example['split'] == 'test' and len(set(example['label']) & set(test_classes)) > 0: test_data.append(example)\n",
    "    else: \n",
    "        for example in all_data:\n",
    "            if example['split'] == 'train' and example['label'] in train_classes: train_data.append(example)\n",
    "            if example['split'] == 'val' and example['label'] in val_classes: val_data.append(example)\n",
    "            if example['split'] == 'test' and example['label'] in test_classes: test_data.append(example)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def _del_by_idx(array_list, idx, axis):\n",
    "    '''\n",
    "        Delete the specified index for each array in the array_lists\n",
    "\n",
    "        @params: array_list: list of np arrays\n",
    "        @params: idx: list of int\n",
    "        @params: axis: int\n",
    "\n",
    "        @return: res: tuple of pruned np arrays\n",
    "    '''\n",
    "    if type(array_list) is not list:\n",
    "        array_list = [array_list]\n",
    "\n",
    "    # modified to perform operations in place\n",
    "    for i, array in enumerate(array_list):\n",
    "        array_list[i] = np.delete(array, idx, axis)\n",
    "\n",
    "    if len(array_list) == 1:\n",
    "        return array_list[0]\n",
    "    else:\n",
    "        return array_list\n",
    "    \n",
    "    \n",
    "def _data_to_nparray(data, vocab, args):\n",
    "    '''\n",
    "        Convert the data into a dictionary of np arrays for speed.\n",
    "    '''\n",
    "    doc_label = np.array([x['label'] for x in data], dtype=np.int64)\n",
    "\n",
    "    raw = np.array([e['text'] for e in data], dtype=object)\n",
    "\n",
    "    # compute the max text length\n",
    "    text_len = np.array([len(m) for e in data for m in e['text']])\n",
    "    max_text_len = max(text_len)\n",
    "    seq_len = np.array(  [len(e['text']) for e in data]  )\n",
    "    max_seq_len =  max(seq_len)\n",
    "    ids = np.array([e['id'] for e in data])\n",
    "\n",
    "    # initialize the big numpy array by <pad>\n",
    "    text = vocab.stoi['<pad>'] * np.ones([len(data), max_seq_len, max_text_len], dtype=np.int64)\n",
    "    \n",
    "    del_idx = []\n",
    "    # convert each token to its corresponding id\n",
    "    for i in tqdm(range(len(data)), desc='converting tokens to ids'): # 모든 대화를 돌면서 한 대화 뭉텅이씩 처리\n",
    "        for idx_x, x in enumerate(data[i]['text']): # 대화 내 모든 문장을 돌면서 한 문장씩 처리\n",
    "                for idx_message, message in enumerate(x): # 각 문장 내 모든 토큰을 돌면서 각 토큰별 vocab을 이용하여 id로 변환\n",
    "                        if message in vocab.stoi:\n",
    "                                text[i, idx_x, idx_message] = vocab.stoi[message]\n",
    "                        else:\n",
    "                                text[i, idx_x, idx_message] = vocab.stoi['<unk>']\n",
    "            # try:\n",
    "            #     for idx_message, message in enumerate(x):\n",
    "            #         text[i, idx_x, :len(message)] = [\n",
    "            #                             vocab.stoi[token] if token in vocab.stoi else vocab.stoi['<unk>'] \n",
    "            #                             for token in message\n",
    "            #                             ]\n",
    "            # except Exception as e:\n",
    "            #     print(e)\n",
    "            #     print(x, idx_x)\n",
    "            #     exit()\n",
    "\n",
    "        # filter out document with only unk and pad\n",
    "        if np.max(text[i]) < 2:\n",
    "            del_idx.append(i)\n",
    "\n",
    "    vocab_size = vocab.vectors.size()[0]\n",
    "    \n",
    "    print(\"del_idx: \", del_idx) # 빈 리스트 반환됨\n",
    "    \n",
    "    ## Curation for padding (string instead of list of list)\n",
    "    raw = [ [\"<pad>\" if m == [\"<pad>\", \"<pad>\", \"<pad>\", \"<pad>\", \"<pad>\"] else m for m in c ] for c in raw ]\n",
    "\n",
    "    if args.authors:\n",
    "        # trim and pad authors (should have been done in dtaa creation but left here for comparison purposes)\n",
    "        authors = list()\n",
    "        for x in data:\n",
    "            a = len(x['authors'])\n",
    "            if a < args.context_size: \n",
    "                authors.append(x['authors'] + [0 for i in range(18-a)])\n",
    "            elif a > args.context_size:\n",
    "                authors.append( x['authors'][int(-args.context_size):] )\n",
    "            else:\n",
    "                authors.append(x['authors'])\n",
    "        authors = np.array(authors, dtype=np.int64)\n",
    "\n",
    "        ids, text_len, text, doc_label, raw, authors = _del_by_idx(\n",
    "                [ids, text_len, text, doc_label, raw, authors], del_idx, 0)\n",
    "        new_data = {\n",
    "            'ids': ids,\n",
    "            'text': text,\n",
    "            'text_len': text_len,\n",
    "            'label': doc_label,\n",
    "            'raw': raw,\n",
    "            'authors': authors,\n",
    "            'vocab_size': vocab_size,\n",
    "        }\n",
    "        \n",
    "    else: ## authors = False\n",
    "        #ids, text_len, text, doc_label, raw = _del_by_idx( [ids, text_len, text, doc_label, raw], del_idx, 0)\n",
    "        new_data = {\n",
    "            'ids': ids,\n",
    "            'text': text,\n",
    "            'text_len': text_len,\n",
    "            'label': doc_label,\n",
    "            'raw': raw,\n",
    "            'vocab_size': vocab_size,\n",
    "        }\n",
    "    return new_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dailydialog_DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        train_classes, val_classes, test_classes = _get_dailydialog_seq_classes()\n",
    "        assert(len(train_classes) == args.n_train_class)\n",
    "        assert(len(val_classes) == args.n_val_class)\n",
    "        assert(len(test_classes) == args.n_test_class)\n",
    "        \n",
    "        all_data = _load_json_seq(self.args.data_path, self.args)\n",
    "        \n",
    "        # Loading word vector\n",
    "        path = os.path.join(self.args.wv_path, self.args.word_vector)\n",
    "        if not os.path.exists(path):\n",
    "            # Download the word vector and save it locally:\n",
    "            print('Downloading word vectors')\n",
    "            import urllib.request\n",
    "            urllib.request.urlretrieve(\n",
    "                'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec',\n",
    "                path)\n",
    "        \n",
    "        vectors = Vectors(args.word_vector, cache=args.wv_path)\n",
    "        min_freq = 2\n",
    "        vocab = Vocab(collections.Counter(_read_words(all_data, convmode=args.convmode)), vectors=vectors,\n",
    "                  specials=['<pad>', '<unk>'], min_freq=min_freq)\n",
    "        \n",
    "         # print word embedding statistics\n",
    "        wv_size = vocab.vectors.size()\n",
    "        print('Total num. of words: {}, word vector dimension: {}'.format(\n",
    "            wv_size[0],\n",
    "            wv_size[1]))\n",
    "        \n",
    "        num_oov = wv_size[0] - torch.nonzero(\n",
    "            torch.sum(torch.abs(vocab.vectors), dim=1)).size()[0]\n",
    "        print(('Num. of out-of-vocabulary words'\n",
    "              '(they are initialized to zeros): {}').format( num_oov))\n",
    "        \n",
    "        # Split into meta-train, meta-val, meta-test data (or just splits)\n",
    "        train_data, val_data, test_data = _meta_split_by_field(all_data, train_classes ,val_classes, test_classes, seqmode=True)\n",
    "        trainset = Counter([l for d in train_data for l in d['label']])\n",
    "        valset = Counter([l for d in val_data for l in d['label']])\n",
    "        testset = Counter([l for d in test_data for l in d['label']])\n",
    "        print(colored('check sets splits', 'yellow'), trainset, len(list(trainset.keys())), valset, len(list(valset.keys())),  testset, len(list(testset.keys())))\n",
    "        \n",
    "        # Convert everything into np array for fast data loading\n",
    "        train_data = _data_to_nparray(train_data, vocab, args)\n",
    "        val_data = _data_to_nparray(val_data, vocab, args)\n",
    "        test_data = _data_to_nparray(test_data, vocab, args)\n",
    "\n",
    "        train_data['is_train'] = True\n",
    "        \n",
    "        #return converted_train_data, converted_val_data, converted_test_data, train_data, val_data, test_data, vocab\n",
    "        return train_data, val_data, test_data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 441723, 4: 12885, 6: 1823, 3: 174, 2: 353, 5: 1150, 1: 1022}\n",
      "Avg len: 35.0\n",
      "Max len: 35\n",
      "Total num. of words: 13967, word vector dimension: 300\n",
      "Num. of out-of-vocabulary words(they are initialized to zeros): 1947\n",
      "\u001b[33mcheck sets splits\u001b[0m Counter({0: 374103, 4: 11182, 6: 1600, 5: 969, 1: 827, 2: 303, 3: 146}) 7 Counter({0: 34039, 4: 684, 6: 107, 5: 79, 1: 77, 3: 11, 2: 3}) 7 Counter({0: 33581, 4: 1019, 1: 118, 6: 116, 5: 102, 2: 47, 3: 17}) 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "converting tokens to ids: 100%|██████████| 11118/11118 [00:01<00:00, 8203.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "del_idx:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "converting tokens to ids: 100%|██████████| 1000/1000 [00:00<00:00, 9460.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "del_idx:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "converting tokens to ids: 100%|██████████| 1000/1000 [00:00<00:00, 9086.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "del_idx:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "loader = dailydialog_DataLoader(args)\n",
    "#converted_train_data, converted_val_data, converted_test_data, train_data, val_data, test_data, vocab = loader.load_dataset()\n",
    "train_data, val_data, test_data, vocab = loader.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # 파일로 저장\n",
    "# with open('/home/hyuns6100/Mental-Heatlh-Care/onnx/vocab.pkl', 'wb') as f:\n",
    "#     pickle.dump(vocab, f)\n",
    "    \n",
    "# # 파일 불러오기\n",
    "# with open('/home/hyuns6100/Mental-Heatlh-Care/onnx/vocab.pkl', 'rb') as f:\n",
    "#     vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "copy_test_data = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 35, 30)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_test_data['text'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 토큰화 예시 확인\n",
    "# data = train_data\n",
    "# text_len = np.array([len(m) for e in data for m in e['text']])\n",
    "# max_text_len = max(text_len)\n",
    "# seq_len = np.array(  [len(e['text']) for e in data]  )\n",
    "# max_seq_len =  max(seq_len)\n",
    "# ids = np.array([e['id'] for e in data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = vocab.stoi['<pad>'] * np.ones([len(data), max_seq_len, max_text_len], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(len(data)), desc='converting tokens to ids'): # 모든 대화를 돌면서 한 대화 뭉텅이씩 처리\n",
    "#         for idx_x, x in enumerate(data[i]['text']): # 대화 내 모든 문장을 돌면서 한 문장씩 처리\n",
    "#                 for idx_message, message in enumerate(x): # 각 문장 내 모든 토큰을 돌면서 각 토큰별 vocab을 이용하여 id로 변환\n",
    "#                         print(message)\n",
    "#                         if message in vocab.stoi:\n",
    "#                                 text[i, idx_x, idx_message] = vocab.stoi[message]\n",
    "#                         else:\n",
    "#                                 text[i, idx_x, idx_message] = vocab.stoi['<unk>']\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text.shape # i번째 대화 뭉텅이, i번째 문장, i번째 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_label = np.array([x['label'] for x in data], dtype=np.int64)\n",
    "\n",
    "# raw = np.array([e['text'] for e in data], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_len = np.array([len(m) for e in data for m in e['text']])\n",
    "# max_text_len = max(text_len)\n",
    "# seq_len = np.array(  [len(e['text']) for e in data]  )\n",
    "# max_seq_len =  max(seq_len)\n",
    "# ids = np.array([e['id'] for e in data])\n",
    "\n",
    "# # initialize the big numpy array by <pad>\n",
    "# text = vocab.stoi['<pad>'] * np.ones([len(data), max_seq_len, max_text_len], dtype=np.int64)\n",
    "\n",
    "# del_idx = []\n",
    "# # convert each token to its corresponding id\n",
    "# for i in tqdm(range(len(data)), desc='converting tokens to ids'):\n",
    "#     for idx_x, x in enumerate(data[i]['text']):\n",
    "#         for idx_message, message in enumerate(x):\n",
    "#                 text[i, idx_x, :len(message)] = [\n",
    "#                                     vocab.stoi[token] if token in vocab.stoi else vocab.stoi['<unk>'] \n",
    "#                                     for token in message\n",
    "#                                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedDataset(Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        '''\n",
    "            data : dict_keys(['ids', 'text', 'text_len', 'label', 'raw', 'vocab_size', 'is_train']) 'authors'\n",
    "        '''\n",
    "        # self.berttokenizer = berttokenizer\n",
    "        # if self.berttokenizer:\n",
    "        #     self.tokenizer = AutoTokenizer.from_pretrained(os.path.join(args.pretrained_bert))\n",
    "        self.args = args\n",
    "        self.ids = data['ids']\n",
    "        self.text = data['text']\n",
    "        self.text_len = data['text_len']\n",
    "        self.label = data['label']\n",
    "        self.raw = data['raw']\n",
    "        #self.authors = data['authors']\n",
    "        self.vocab_size = data['vocab_size']\n",
    "        self.train = False\n",
    "        if 'is_train' in data:\n",
    "            self.is_train = data['is_train']\n",
    "            self.train = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        item = {\n",
    "            'ids': self.ids[idx], \n",
    "            'text': self.text[idx], \n",
    "            'text_len': self.text_len[idx], \n",
    "            # 'label': np.expand_dims(self.label[idx],0),  # .expand_dims(x, axis=0) unsqueeze(0) for seq labelling (bert)\n",
    "            'label': self.label[idx],\n",
    "            # 'raw': self.raw[idx].tolist(), \n",
    "            'vocab_size': self.vocab_size,\n",
    "            #'authors': self.authors[idx]\n",
    "        }\n",
    "\n",
    "       \n",
    "        if self.train: item.update({'is_train': self.is_train})\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data_utils.DataLoader(SupervisedDataset(train_data, args), batch_size=args.batch_size, num_workers=2, shuffle=False)\n",
    "val_loader = data_utils.DataLoader(SupervisedDataset(val_data, args), batch_size=args.batch_size, num_workers=2, shuffle=False)\n",
    "test_loader = data_utils.DataLoader(SupervisedDataset(test_data, args), batch_size=args.batch_size, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WORDEBD(nn.Module):\n",
    "    '''\n",
    "        An embedding layer that maps the token id into its corresponding word\n",
    "        embeddings. The word embeddings are kept as fixed once initialized.\n",
    "    '''\n",
    "    def __init__(self, vocab, finetune_ebd):#, specific_vocab_size=None):\n",
    "        super(WORDEBD, self).__init__()\n",
    "\n",
    "        self.vocab_size, self.embedding_dim = vocab.vectors.size()\n",
    "        # if specific_vocab_size != None: self.vocab_size = specific_vocab_size\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "                self.vocab_size, self.embedding_dim)\n",
    "        self.embedding_layer.weight.data = vocab.vectors\n",
    "\n",
    "        self.finetune_ebd = finetune_ebd\n",
    "\n",
    "        if self.finetune_ebd:\n",
    "            self.embedding_layer.weight.requires_grad = True\n",
    "        else:\n",
    "            self.embedding_layer.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, data, weights=None):\n",
    "        '''\n",
    "            @param text: batch_size * max_text_len\n",
    "            @return output: batch_size * max_text_len * embedding_dim\n",
    "        '''\n",
    "        if (weights is None): #or (self.finetune_ebd == False):\n",
    "            return self.embedding_layer(data['text'])\n",
    "\n",
    "        else:\n",
    "            return F.embedding(data['text'],\n",
    "                               weights['ebd.embedding_layer.weight'])\n",
    "\n",
    "class CNNseq(nn.Module):\n",
    "    '''\n",
    "        An aggregation method that encodes every document through different\n",
    "        convolution filters (followed by max-over-time pooling).\n",
    "    '''\n",
    "    def __init__(self, ebd, args):\n",
    "        super(CNNseq, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.ebd = ebd # pre-trained FastText로 initialization된 token representation => WORDEBD => nn.Embedding layer로 매핑된 것\n",
    "\n",
    "        self.input_dim = self.ebd.embedding_dim\n",
    "\n",
    "        # Convolution\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(\n",
    "                    in_channels=self.input_dim,\n",
    "                    out_channels=args.cnn_num_filters,\n",
    "                    kernel_size=K) for K in args.cnn_filter_sizes])\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.ebd_dim = args.cnn_num_filters * len(args.cnn_filter_sizes)\n",
    "\n",
    "    def _conv_max_pool(self, x, conv_filter=None, weights=None):\n",
    "        '''\n",
    "        Compute sentence level convolution\n",
    "        Input:\n",
    "            x:      batch_size, max_doc_len, embedding_dim\n",
    "        Output:     batch_size, num_filters_total\n",
    "        '''\n",
    "        assert(len(x.size()) == 3) # [batch_size==max_sentences, max_tokens, embedding_dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # batch_size, embedding_dim, doc_len\n",
    "        x = x.contiguous()\n",
    "\n",
    "        # Apply the 1d conv. Resulting dimension is\n",
    "        # [batch_size, num_filters, doc_len-filter_size+1] * len(filter_size)\n",
    "        assert(not ((conv_filter is None) and (weights is None)))\n",
    "        if conv_filter is not None:\n",
    "            x = [conv(x) for conv in conv_filter]\n",
    "\n",
    "        # elif weights is not None:\n",
    "        #     x = [F.conv1d(x, weight=weights['convs.{}.weight'.format(i)],\n",
    "        #                 bias=weights['convs.{}.bias'.format(i)])\n",
    "        #         for i in range(len(self.args.cnn_filter_sizes))]\n",
    "\n",
    "        ## max pool over time. Resulting dimension is\n",
    "        ## [batch_size, num_filters] * len(filter_size)\n",
    "        #x = [F.max_pool1d(sub_x, sub_x.size(2)).squeeze(2) for sub_x in x]\n",
    "        \n",
    "        ## nn.MaxPool1d로 다시 생성\n",
    "        max_pooled_outputs = []\n",
    "        for sub_x in x:\n",
    "            pool_size = sub_x.size(2)\n",
    "            max_pool = nn.MaxPool1d(pool_size) # output shape: [batch_size, num_filters, 1]\n",
    "            pooled = max_pool(sub_x).squeeze(2) # output shape: [batch_size, num_filters]\n",
    "            max_pooled_outputs.append(pooled)\n",
    "        \n",
    "        # concatenate along all filters. Resulting dimension is\n",
    "        # output: [batch_size, num_filters_total]\n",
    "        x = torch.cat(max_pooled_outputs, 1) # output shape: [batch_size, num_filters*3]\n",
    "        #x = torch.cat(x, 1)\n",
    "        x = self.relu(x) #F.relu(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, data, weights=None):\n",
    "        '''\n",
    "            @param data dictionary\n",
    "                @key text: batch_size * max_text_len\n",
    "            @param weights placeholder used for maml\n",
    "\n",
    "            @return output: batch_size * embedding_dim\n",
    "        '''\n",
    "\n",
    "        device = data['text'].device\n",
    "        \n",
    "        # Apply the word embedding, result:  batch_size, doc_len, embedding_dim\n",
    "        \n",
    "        ebd = self.ebd(data, weights) # ouptut: [batch_size, max_sentences, max_tokens, embedding_dim]\n",
    "\n",
    "        # apply 1d conv + max pool, result:  batch_size, num_filters_total        \n",
    "        ref = tuple(data['text'].size())\n",
    "        shape = (ref[0], ref[1], ( len(self.args.cnn_filter_sizes) * self.args.cnn_num_filters))\n",
    "        output = torch.randn(shape).to(device)\n",
    "        \n",
    "        if weights is None:\n",
    "            for i in range(ebd.size(0)): # 각 배치에 대해\n",
    "                out = self._conv_max_pool(ebd[i], conv_filter=self.convs) # 각 문장에 대해 처리 => (35, 300)\n",
    "                output[i] = out\n",
    "        \n",
    "        else:\n",
    "            for i in range(ebd.size(0)):\n",
    "                out = self._conv_max_pool(ebd[i], weights=weights)\n",
    "                output[i] = out\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def get_embedding(vocab, args):\n",
    "    ebd = WORDEBD(vocab, args.finetune_ebd)\n",
    "    model = CNNseq(ebd, args)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # output = out_XS.view(-1, args.n_classes)  # new shape: [32*35, 7]\n",
    "# # target = YS.view(-1)  # new shape: [32*35]\n",
    "\n",
    "# output = torch.rand((32*35, 7))\n",
    "# target = test_data['label'][:32].view()\n",
    "\n",
    "# output.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in test_loader:\n",
    "#     YS = batch['label']\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1120])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YS = batch['label']\n",
    "# target = YS.view(-1)\n",
    "# target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15267856419086456"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.mean((torch.argmax(output, dim=1) == target).float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1527)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.mean((torch.argmax(output, dim=1) == target).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1527)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (torch.argmax(output, dim=1) == target).float().sum() / len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class distLinear(nn.Module):\n",
    "    def __init__(self, indim, outdim):\n",
    "        super(distLinear, self).__init__()\n",
    "        self.L = nn.Linear(indim, outdim, bias = False)\n",
    "        # split the weight update component to direction and norm\n",
    "        # WeightNorm.apply(self.L, 'weight', dim=0)\n",
    "\n",
    "        # a fixed scale factor to scale the output of cos value\n",
    "        # into a reasonably large input for softmax\n",
    "        self.scale_factor = 10\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_norm = torch.norm(x, p=2, dim =1).unsqueeze(1).expand_as(x)\n",
    "        x_normalized = x.div(x_norm + 0.00001)\n",
    "        # L_norm = torch.norm(self.L.weight.data, p=2, dim=1).unsqueeze(1).expand_as(self.L.weight.data)\n",
    "\n",
    "        # self.L.weight.data = self.L.weight.data.div(L_norm + 0.00001)\n",
    "\n",
    "        cos_dist = self.L(x_normalized)  # matrix product by forward function\n",
    "        scores = self.scale_factor * (cos_dist)\n",
    "\n",
    "        return scores\n",
    "    \n",
    "class BASE(nn.Module):\n",
    "    '''\n",
    "        BASE model\n",
    "    '''\n",
    "    def __init__(self, args):\n",
    "        super(BASE, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        # cached tensor for speed\n",
    "        # self.I_way = nn.Parameter(torch.eye(self.args.way, dtype=torch.float),\n",
    "        #                           requires_grad=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_acc(pred, true, dim=1, nomax=False):\n",
    "        '''\n",
    "            Compute the accuracy.\n",
    "            @param pred: batch_size * num_classes\n",
    "            @param true: batch_size\n",
    "        '''\n",
    "        if nomax: return torch.mean((pred == true).float()).item()\n",
    "        else: return torch.mean((torch.argmax(pred, dim=dim) == true).float()).item()\n",
    "        \n",
    "    @staticmethod\n",
    "    def compute_f1(y_pred, true, dim=1, nomax=False,  labels=None, average='weighted'):\n",
    "        '''\n",
    "            Compute the weighted f1 score.\n",
    "            @param pred: batch_size * num_classes\n",
    "            @param true: batch_size\n",
    "        '''\n",
    "        if not nomax: _, y_pred = torch.max(y_pred, dim)\n",
    "\n",
    "        f1 = f1_score(true.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average=average, labels=labels)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_mcc(y_pred, true, dim=1, nomax=False):\n",
    "        '''\n",
    "            Compute the matthews correlation coeficient.\n",
    "            @param pred: batch_size * num_classes\n",
    "            @param true: batch_size\n",
    "        '''\n",
    "        if not nomax: _, y_pred = torch.max(y_pred, dim)\n",
    "\n",
    "        mcc = matthews_corrcoef(true.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n",
    "\n",
    "        return mcc\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_f1_micro_noneutral(y_pred, true, dim=1, nomax=False, labels=None):\n",
    "        \n",
    "        if not nomax: _, y_pred = torch.max(y_pred, dim)\n",
    "\n",
    "        f1 = f1_score(true.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average='micro', labels=labels)\n",
    "\n",
    "        return f1\n",
    "\n",
    "class MLPseq(BASE):\n",
    "    def __init__(self, ebd_dim, args, top_layer=None):\n",
    "        super(MLPseq, self).__init__(args)\n",
    "\n",
    "        self.args = args\n",
    "        self.ebd_dim = ebd_dim\n",
    "\n",
    "        self.mlp = self._init_mlp(ebd_dim, self.args.mlp_hidden, self.args.dropout)\n",
    "        self.out = self.get_top_layer(self.args, self.args.n_classes)\n",
    "        #self.top_layer = top_layer\n",
    "        self.dropout = nn.Dropout(self.args.dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_top_layer(args, n_classes):\n",
    "        '''\n",
    "            Creates final layer of desired type\n",
    "            @return final classification layer\n",
    "        '''\n",
    "        return nn.Linear(args.mlp_hidden[-1], n_classes)\n",
    "\n",
    "        \n",
    "    def _init_mlp(self, in_d, hidden_ds, drop_rate):\n",
    "        modules = []\n",
    "\n",
    "        for d in hidden_ds[:-1]:\n",
    "            modules.extend([\n",
    "                nn.Dropout(drop_rate),\n",
    "                nn.Linear(in_d, d),\n",
    "                nn.ReLU()])\n",
    "            in_d = d\n",
    "\n",
    "        modules.extend([\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(in_d, hidden_ds[-1])])\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, XS, YS=None, XQ=None, YQ=None, weights=None, return_preds=False):\n",
    "        '''\n",
    "            if y is specified, return loss and accuracy\n",
    "            otherwise, return the transformed x\n",
    "\n",
    "            @param: XS: batch_size * input_dim\n",
    "            @param: YS: batch_size (optional)\n",
    "\n",
    "            @return: XS: batch_size * output_dim\n",
    "        '''\n",
    "\n",
    "        # normal training procedure, train stage only use query\n",
    "        # if weights is None:\n",
    "        #     XS = self.mlp(XS)\n",
    "        # else:\n",
    "        #     # find weight and bias keys for the mlp module\n",
    "        #     w_keys, b_keys = [], []\n",
    "        #     for key in weights.keys():\n",
    "        #         if key[:4] == 'mlp.':\n",
    "        #             if key[-6:] == 'weight':\n",
    "        #                 w_keys.append(key)\n",
    "        #             else:\n",
    "        #                 b_keys.append(key)\n",
    "\n",
    "        #     for i in range(len(w_keys)-1):\n",
    "        #         #XS = F.dropout(XS, self.args.dropout, training=self.training)\n",
    "        #         XS = self.dropout(XS)\n",
    "        #         XS = F.linear(XS, weights[w_keys[i]], weights[b_keys[i]])\n",
    "        #         XS = F.relu(XS)\n",
    "\n",
    "        #     XS = F.dropout(XS, self.args.dropout, training=self.training)\n",
    "        #     XS = F.linear(XS, weights[w_keys[-1]], weights[b_keys[-1]])\n",
    "\n",
    "        XS = self.mlp(XS)\n",
    "        XS = self.out(XS) # output: [batch, max_sentence, n_class]\n",
    "        \n",
    "        # if self.top_layer is not None:\n",
    "        #     XS = self.top_layer(XS)\n",
    "\n",
    "        # # normal training procedure, compute loss/acc\n",
    "        # if YS is not None:\n",
    "        #     # if self.args.taskmode == 'episodic':\n",
    "        #     #     ## useful for episodes, ignored for full supervised\n",
    "        #     #     _, YS = torch.unique(YS, sorted=True, return_inverse=True)\n",
    "        #     loss = F.cross_entropy(XS, YS)\n",
    "        #     acc = BASE.compute_acc(XS, YS)\n",
    "        #     f1 = BASE.compute_f1(XS, YS)\n",
    "        #     mcc = BASE.compute_mcc(XS, YS)\n",
    "\n",
    "        #     if return_preds:\n",
    "        #         _, y_pred = torch.max(XS, dim=1)\n",
    "        #         return acc, loss, f1, mcc, y_pred, YS\n",
    "        #     else:\n",
    "        #         return acc, loss, f1, mcc\n",
    "\n",
    "        # else:\n",
    "        #     return XS\n",
    "        \n",
    "        return XS\n",
    "\n",
    "def get_classifier(emb_dim, args):\n",
    "    model = MLPseq(emb_dim, args)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "model[\"ebd\"] = get_embedding(vocab, args)\n",
    "model['ebd'].train()\n",
    "model[\"clf\"] = get_classifier(model[\"ebd\"].ebd_dim, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CNNseq(\n",
       "   (ebd): WORDEBD(\n",
       "     (embedding_layer): Embedding(13967, 300)\n",
       "   )\n",
       "   (convs): ModuleList(\n",
       "     (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
       "     (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
       "     (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
       "   )\n",
       "   (relu): ReLU()\n",
       " ),\n",
       " MLPseq(\n",
       "   (mlp): Sequential(\n",
       "     (0): Dropout(p=0.1, inplace=False)\n",
       "     (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "     (2): ReLU()\n",
       "     (3): Dropout(p=0.1, inplace=False)\n",
       "     (4): Linear(in_features=300, out_features=300, bias=True)\n",
       "   )\n",
       "   (out): Linear(in_features=300, out_features=7, bias=True)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"ebd\"], model[\"clf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grad_param(model, keys):\n",
    "    '''\n",
    "        Return a generator that generates learnable parameters in\n",
    "        model[key] for key in keys.\n",
    "    '''\n",
    "    if len(keys) == 1:\n",
    "        return filter(lambda p: p.requires_grad,\n",
    "                model[keys[0]].parameters())\n",
    "    else:\n",
    "        return filter(lambda p: p.requires_grad,\n",
    "                itertools.chain.from_iterable(\n",
    "                    model[key].parameters() for key in keys))\n",
    "\n",
    "def get_norm(model):\n",
    "    '''\n",
    "        Compute norm of the gradients\n",
    "    '''\n",
    "    total_norm = 0\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            p_norm = p.grad.data.norm()\n",
    "            total_norm += p_norm.item() ** 2\n",
    "\n",
    "    total_norm = total_norm ** 0.5\n",
    "\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, val_data, model, args, loader=None):\n",
    "    '''\n",
    "        Train the model\n",
    "        Use val_data to do early stopping\n",
    "    '''\n",
    "\n",
    "    \n",
    "    out_dir = args.result_path\n",
    "\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_score = 0\n",
    "    sub_cycle = 0\n",
    "    best_path = None\n",
    "\n",
    "    # opt = torch.optim.Adam(grad_param(model, ['ebd', 'clf']), lr=args.lr)\n",
    "    opt = torch.optim.Adam(grad_param(model, ['ebd', 'clf']), lr=args.lr, betas=(0.9, 0.98), eps=pow(10, -9)) # CESTa optimizer parameters\n",
    "\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'max', patience=2, factor=0.1, verbose=True) # args.patience//2\n",
    "\n",
    "    print(\"{}, Start training supervised\".format(datetime.datetime.now().strftime('%02y/%02m/%02d %H:%M:%S')), flush=True)\n",
    "\n",
    "    for ep in range(args.train_epochs):\n",
    "\n",
    "        grad = {'clf': [], 'ebd': []}\n",
    "        \n",
    "        # train on training set\n",
    "        for batch in tqdm(loader, ncols=80, leave=False, desc=colored('Training on train', 'yellow')):\n",
    "            train_one(batch, model, opt, args, grad)\n",
    "\n",
    "        # Evaluate validation accuracy\n",
    "        # cur_acc, cur_std, cur_f1, cur_f1_std, cur_mcc, cur_mcc_std, cur_f1_micro, cur_f1_micro_std = test(val_data, model, args, False)\n",
    "        cur_acc, cur_std, cur_f1, cur_f1_std, cur_mcc, cur_mcc_std, cur_f1_micro, cur_f1_micro_std = test(val_data, model, args, False, loader=val_loader)\n",
    "        print(cur_acc, cur_std, cur_f1, cur_f1_std, cur_mcc, cur_mcc_std)\n",
    "        \n",
    "        print((\"{}, {:s} {:2d}, {:s} {:s}{:>7.4f} ± {:>6.4f}, {:s}{:>7.4f} ± {:>6.4f}, {:s}{:>7.4f} ± {:>6.4f}, {:s}{:>7.4f} ± {:>6.4f},\"\n",
    "               \"{:s} {:s}{:>7.4f}, {:s}{:>7.4f}\").format(\n",
    "               datetime.datetime.now().strftime('%02y/%02m/%02d %H:%M:%S'),\n",
    "               \"ep\", ep,\n",
    "               colored(\"val  \", \"cyan\"),\n",
    "               colored(\"acc:\", \"blue\"), cur_acc, cur_std,\n",
    "               colored(\"f1:\", \"blue\"), cur_f1, cur_f1_std,\n",
    "               colored(\"mcc:\", \"blue\"), cur_mcc, cur_mcc_std,\n",
    "               colored(\"f1 micro:\", \"blue\"), cur_f1_micro, cur_f1_micro_std,\n",
    "               colored(\"train stats\", \"cyan\"),\n",
    "               colored(\"ebd_grad:\", \"blue\"), np.mean(np.array(grad['ebd'])),\n",
    "               colored(\"clf_grad:\", \"blue\"), np.mean(np.array(grad['clf'])),\n",
    "               ), flush=True)\n",
    "        scores = {'acc':cur_acc, 'f1': cur_f1, 'mcc': cur_mcc, 'f1_micro': cur_f1_micro}\n",
    "\n",
    "        # Update the current best model if val acc is better\n",
    "        # if cur_acc > best_acc:\n",
    "        #     best_acc = cur_acc\n",
    "        if scores[args.patience_metric] > best_score:\n",
    "            best_score = scores[args.patience_metric]\n",
    "            best_path = os.path.join(out_dir, str(ep))\n",
    "\n",
    "            print( colored( \"{}, Attempt to save cur best model to {}\".format(\n",
    "                datetime.datetime.now().strftime('%02y/%02m/%02d %H:%M:%S'),\n",
    "                best_path) , 'magenta' ))\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    torch.save(model['ebd'].state_dict(), best_path + '.ebd')\n",
    "                    torch.save(model['clf'].state_dict(), best_path + '.clf')\n",
    "                    break\n",
    "                except (FileNotFoundError):\n",
    "                    continue\n",
    "            \n",
    "            # save current model\n",
    "            print( colored( \"{}, Saved cur best model to {}\".format(\n",
    "                datetime.datetime.now().strftime('%02y/%02m/%02d %H:%M:%S'),\n",
    "                best_path) , 'magenta' ))\n",
    "\n",
    "            sub_cycle = 0\n",
    "        else:\n",
    "            sub_cycle += 1\n",
    "\n",
    "        #if args.scheduler: scheduler.step(cur_acc)\n",
    "\n",
    "        # Break if the val acc hasn't improved in the past patience epochs\n",
    "        if sub_cycle == args.patience:\n",
    "            break\n",
    "\n",
    "    print(\"{}, End of training. Restore the best weights\".format(\n",
    "            datetime.datetime.now().strftime('%02y/%02m/%02d %H:%M:%S')),\n",
    "            flush=True)\n",
    "\n",
    "    # restore the best saved model\n",
    "    while True:\n",
    "        try:\n",
    "            model['ebd'].load_state_dict(torch.load(best_path + '.ebd'))\n",
    "            model['clf'].load_state_dict(torch.load(best_path + '.clf'))\n",
    "            break\n",
    "        except (FileNotFoundError):\n",
    "            continue\n",
    "    \n",
    "\n",
    "    if args.save:\n",
    "        # save the current model\n",
    "        # out_dir = os.path.abspath(os.path.join(\n",
    "        #                               os.path.curdir,\n",
    "        #                               \"saved-runs\",\n",
    "        #                               str(int(time.time() * 1e7))))\n",
    "        out_dir = args.result_text_path\n",
    "        \n",
    "        # if args.result_path != '':\n",
    "        #     dir_path = os.path.split(args.result_path)[0]\n",
    "        #     out_dir = os.path.abspath(os.path.join(\n",
    "        #                               os.path.curdir, dir_path) )\n",
    "\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        best_path = os.path.join(out_dir, 'best')\n",
    "\n",
    "        print(colored(\"{}, Save best model to {}\".format(\n",
    "            datetime.datetime.now().strftime('%02y/%02m/%02d %H:%M:%S'),\n",
    "            best_path), \"green\"), flush=True)\n",
    "\n",
    "        torch.save(model['ebd'].state_dict(), best_path + '.ebd')\n",
    "        torch.save(model['clf'].state_dict(), best_path + '.clf')\n",
    "\n",
    "        with open(best_path + '_args.txt', 'w') as f:\n",
    "            for attr, value in sorted(args.__dict__.items()):\n",
    "                f.write(\"{}={}\\n\".format(attr, value))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def train_one(batch, model, opt, args, grad):\n",
    "    '''\n",
    "        Train the model on one sampled task.\n",
    "    '''\n",
    "    model['ebd'].train()\n",
    "    model['clf'].train()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    batch['text'] = batch['text'].to(device)\n",
    "    batch['label'] = batch['label'].to(device)\n",
    "\n",
    "    XS = model['ebd'](batch)\n",
    "    YS = batch['label']\n",
    "\n",
    "    # if args.classifier == \"cesta\":\n",
    "    #     acc, loss, f1, mcc, f1_micro = model['clf'](XS, YS, None, None, authors=batch['authors'])\n",
    "    # else:\n",
    "    # Apply the classifier (need to be MLP classifier)\n",
    "    #acc, loss, f1, mcc = model['clf'](XS, YS, None, None)\n",
    "\n",
    "    out_XS = model['clf'](XS, None, None, None)\n",
    "    \n",
    "    output = out_XS.view(-1, args.n_classes)  # new shape: [32*35, 7]\n",
    "\n",
    "    # Flatten the target\n",
    "    target = YS.view(-1)  # new shape: [32*35]\n",
    "\n",
    "    loss = F.cross_entropy(output, target)\n",
    "    # acc = BASE.compute_acc(out_XS, YS)\n",
    "    # f1 = BASE.compute_f1(out_XS, YS)\n",
    "    # mcc = BASE.compute_mcc(out_XS, YS)\n",
    "    \n",
    "    if loss is not None:\n",
    "        loss.backward()\n",
    "\n",
    "    if torch.isnan(loss):\n",
    "        return\n",
    "\n",
    "    # if args.clip_grad is not None:\n",
    "    #     nn.utils.clip_grad_value_(grad_param(model, ['ebd', 'clf']), args.clip_grad)\n",
    "    #     # nn.utils.clip_grad_norm_(grad_param(model, ['ebd', 'clf']), args.clip_grad) #0.5\n",
    "\n",
    "    grad['clf'].append(get_norm(model['clf']))\n",
    "    grad['ebd'].append(get_norm(model['ebd']))\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "def test(test_data, model, args, verbose=True, target='val', loader=None):\n",
    "    '''\n",
    "        Evaluate the model on a bag of sampled tasks. Return the mean accuracy, \n",
    "        the weighted f1 score and the matthew correlation coeficient and their\n",
    "        associated std. (ensure the model used is modified to return the values)\n",
    "    '''\n",
    "    model['ebd'].eval()\n",
    "    model['clf'].eval()\n",
    "\n",
    "    acc, f1, mcc, f1_micro, trues, preds = [], [], [], [], [], []\n",
    "    # if loader is None:\n",
    "    #     loader = DataLoader(SupervisedDataset(test_data, args), batch_size=args.batch_size, num_workers=2, shuffle=False)\n",
    "\n",
    "    for batch in tqdm(loader, desc=colored('Testing regular on %s' % (target), 'yellow'), total=loader.__len__()):\n",
    "        #res_acc, res_f1, res_mcc, res_f1_micro, res_pred, res_true = test_one(batch, model, args, out=(target=='test'))\n",
    "        res_acc, res_f1, res_mcc, res_f1_micro = test_one(batch, model, args, out=(target=='test'))\n",
    "        acc.append(res_acc)\n",
    "        f1.append(res_f1)\n",
    "        mcc.append(res_mcc)\n",
    "        f1_micro.append(res_f1_micro)\n",
    "        # trues.extend(res_true.cpu().detach().tolist())\n",
    "        # preds.extend(res_pred.cpu().detach().tolist())\n",
    "\n",
    "    acc, f1, mcc, f1_micro = np.array(acc), np.array(f1), np.array(mcc), np.array(f1_micro)\n",
    "\n",
    "    # if target == 'test' and args.dataset == 'ouitchat_seq':\n",
    "    #     target_names = ['no emotion', 'anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']\n",
    "    #     labels = [0, 1, 2, 3, 4, 5, 6]\n",
    "    #     print(confusion_matrix(np.array(trues), np.array(preds), labels=labels))\n",
    "    #     print(classification_report(np.array(trues), np.array(preds), labels=labels, target_names=target_names ) )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"{}, {:s} {:>7.4f} ({:s} {:>7.4f}), {:s} {:>7.4f} ({:s} {:>7.4f}), {:s} {:>7.4f} ({:s} {:>7.4f}), {:s} {:>7.4f} ({:s} {:>7.4f})\".format(\n",
    "                datetime.datetime.now().strftime('%02y/%02m/%02d %H:%M:%S'),\n",
    "                colored(\"acc mean\", \"blue\"),\n",
    "                np.mean(acc),\n",
    "                colored(\"std\", \"blue\"),\n",
    "                np.std(acc),\n",
    "                colored(\"f1 mean\", \"blue\"),\n",
    "                np.mean(f1),\n",
    "                colored(\"std\", \"blue\"),\n",
    "                np.std(f1),\n",
    "                colored(\"mcc mean\", \"blue\"),\n",
    "                np.mean(mcc),\n",
    "                colored(\"std\", \"blue\"),\n",
    "                np.std(mcc),\n",
    "                colored(\"f1 micro mean\", \"blue\"),\n",
    "                np.mean(f1_micro),\n",
    "                colored(\"std\", \"blue\"),\n",
    "                np.std(f1_micro),\n",
    "                ), flush=True)\n",
    "\n",
    "        # latex table\n",
    "        print(\"{:s} & {:s} & {:>7.4f} \\\\tiny $\\\\pm {:>7.4f}$ & {:>7.4f} \\\\tiny $\\\\pm {:>7.4f}$ & {:>7.4f} \\\\tiny $\\\\pm {:>7.4f}$ & {:>7.4f} \\\\tiny $\\\\pm {:>7.4f}$\".format(\n",
    "                args.embedding.replace('_', '\\\\_'),\n",
    "                args.classifier.replace('_', '\\\\_'),\n",
    "                np.mean(acc),\n",
    "                np.std(acc),\n",
    "                np.mean(f1),\n",
    "                np.std(f1),\n",
    "                np.mean(mcc),\n",
    "                np.std(mcc),\n",
    "                np.mean(f1_micro),\n",
    "                np.std(f1_micro),\n",
    "                ), flush=True)\n",
    "    # if args.classifier == 'cesta':\n",
    "    #     return np.mean(acc), np.std(acc), np.mean(f1), np.std(f1), np.mean(mcc), np.std(mcc), np.mean(f1_micro), np.std(f1_micro)\n",
    "    return np.mean(acc), np.std(acc), np.mean(f1), np.std(f1), np.mean(mcc), np.std(mcc), np.mean(f1_micro), np.std(f1_micro)\n",
    "\n",
    "\n",
    "def test_one(batch, model, args, out=False):\n",
    "    '''\n",
    "        Evaluate the model on one sampled task. Return the accuracy.\n",
    "    '''\n",
    "\n",
    "    batch['text'] = batch['text'].to(device)\n",
    "    batch['label'] = batch['label'].to(device)\n",
    "\n",
    "    # Embedding the document\n",
    "    XS = model['ebd'](batch)\n",
    "    YS = batch['label']\n",
    "\n",
    "    # # Apply the classifier\n",
    "    # if args.dump and out:\n",
    "    #     acc, loss, f1, mcc = model['clf'](XS, YS=YS, out=out, XS_ids=batch['ids'])\n",
    "    # elif out and args.classifier != 'cesta':\n",
    "    #     acc, loss, f1, mcc, y_pred, y_true = model['clf'](XS, YS=YS, return_preds=True)\n",
    "    #     return acc, f1, mcc, y_pred, y_true\n",
    "    # else:\n",
    "    #     if args.classifier == 'cesta': \n",
    "    #         acc, loss, f1, mcc, f1_micro, y_pred, y_true = model['clf'](XS, YS=YS, authors=batch['authors'], return_preds=True)\n",
    "    #         return acc, f1, mcc, f1_micro, y_pred, y_true\n",
    "    #     else: acc, loss, f1, mcc = model['clf'](XS, YS=YS)\n",
    "\n",
    "    out_XS = model['clf'](XS, YS=None)\n",
    "\n",
    "    output = out_XS.view(-1, args.n_classes)  # new shape: [32*35, 7]\n",
    "    target = YS.view(-1)  # new shape: [32*35]\n",
    "\n",
    "    #loss = F.cross_entropy(output, YS)\n",
    "    acc = BASE.compute_acc(output, target)\n",
    "    f1 = BASE.compute_f1(output, target)\n",
    "    mcc = BASE.compute_mcc(output, target)\n",
    "    micro_f1_noneutral = BASE.compute_f1_micro_noneutral(output, target, labels=args['labels'])\n",
    "    \n",
    "    return acc, f1, mcc, micro_f1_noneutral\n",
    "    \n",
    "    # if out : #and args.classifier != 'cesta':\n",
    "    #     acc, loss, f1, mcc, y_pred, y_true = model['clf'](XS, YS=YS, return_preds=True)\n",
    "    #     return acc, f1, mcc, y_pred, y_true\n",
    "    # else:\n",
    "    #     # if args.classifier == 'cesta': \n",
    "    #     #     acc, loss, f1, mcc, f1_micro, y_pred, y_true = model['clf'](XS, YS=YS, authors=batch['authors'], return_preds=True)\n",
    "    #     #     return acc, f1, mcc, f1_micro, y_pred, y_true\n",
    "    #     #else: acc, loss, f1, mcc = model['clf'](XS, YS=YS)\n",
    "    #     acc, loss, f1, mcc = model['clf'](XS, YS=YS)\n",
    "\n",
    "    # return acc, f1, mcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 예시\n",
    "# model['ebd'].train()\n",
    "# model['clf'].train()\n",
    "\n",
    "# batch_ = train_loader.dataset[0:2]\n",
    "# batch_['text'] = torch.tensor(batch_['text']).to(device)\n",
    "# batch_['label'] = torch.tensor(batch_['label']).to(device)\n",
    "\n",
    "# batch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordebd = WORDEBD(vocab, args.finetune_ebd).to(device)\n",
    "# batch_['text'] = batch_['text'].to(device)\n",
    "# batch_['label'] = batch_['label'].to(device)\n",
    "# ebd_xs_ = wordebd(batch_)\n",
    "# ebd_xs_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnnebd = CNNseq(wordebd, args).to(device)\n",
    "# batch_['text'] = batch_['text'].to(device)\n",
    "# batch_['label'] = batch_['label'].to(device)\n",
    "# cnn_xs_ = cnnebd(batch_)\n",
    "# cnn_xs_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs_ = model['ebd'](batch_)\n",
    "# ys_ = batch_['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs_.size(), ys_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_xs = model['clf'](xs_, None, None, None)\n",
    "# out_xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = out_xs.view(-1, 7)  # new shape: [32*35, 7]\n",
    "\n",
    "# # Flatten the target\n",
    "# target = ys_.view(-1)  # new shape: [32*35]\n",
    "\n",
    "# # Compute the loss\n",
    "# loss = F.cross_entropy(output, target)\n",
    "\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/06 18:25:51, Start training supervised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 18.30it/s]8.21it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9769531320780516 0.0053124940998657 0.9729722728874199 0.007060956560666433 0.4852064679405341 0.09166620261637358\n",
      "23/12/06 18:26:16, ep  0, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9770 ± 0.0053, \u001b[34mf1:\u001b[0m 0.9730 ± 0.0071, \u001b[34mmcc:\u001b[0m 0.4852 ± 0.0917, \u001b[34mf1 micro:\u001b[0m 0.4680 ± 0.1017,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.0900, \u001b[34mclf_grad:\u001b[0m 0.1744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m23/12/06 18:26:16, Attempt to save cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/0\u001b[0m\n",
      "\u001b[35m23/12/06 18:26:16, Saved cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 22.79it/s]6.96it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9776506740599871 0.005170125110784994 0.9737174860078077 0.0067637961989397526 0.493303103038907 0.10839100813585993\n",
      "23/12/06 18:26:39, ep  1, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9777 ± 0.0052, \u001b[34mf1:\u001b[0m 0.9737 ± 0.0068, \u001b[34mmcc:\u001b[0m 0.4933 ± 0.1084, \u001b[34mf1 micro:\u001b[0m 0.4697 ± 0.1165,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.0755, \u001b[34mclf_grad:\u001b[0m 0.1037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m23/12/06 18:26:39, Attempt to save cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/1\u001b[0m\n",
      "\u001b[35m23/12/06 18:26:39, Saved cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 22.43it/s]7.04it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977706478908658 0.005317403492100663 0.9733068175284254 0.007062193021633683 0.4846569071239556 0.1006669404375958\n",
      "23/12/06 18:27:02, ep  2, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9777 ± 0.0053, \u001b[34mf1:\u001b[0m 0.9733 ± 0.0071, \u001b[34mmcc:\u001b[0m 0.4847 ± 0.1007, \u001b[34mf1 micro:\u001b[0m 0.4583 ± 0.1078,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.0886, \u001b[34mclf_grad:\u001b[0m 0.1004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 18.03it/s]6.82it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9782645106315613 0.004939091942789327 0.9744043597971612 0.006449420981850155 0.505822348437678 0.0930139863458389\n",
      "23/12/06 18:27:24, ep  3, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9783 ± 0.0049, \u001b[34mf1:\u001b[0m 0.9744 ± 0.0064, \u001b[34mmcc:\u001b[0m 0.5058 ± 0.0930, \u001b[34mf1 micro:\u001b[0m 0.4753 ± 0.1018,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1023, \u001b[34mclf_grad:\u001b[0m 0.0997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m23/12/06 18:27:24, Attempt to save cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/3\u001b[0m\n",
      "\u001b[35m23/12/06 18:27:24, Saved cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/3\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.38it/s]7.42it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9773995522409678 0.005377743159541637 0.9737095186650961 0.006792284979471801 0.48471915087422685 0.08984527676240218\n",
      "23/12/06 18:27:46, ep  4, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9774 ± 0.0054, \u001b[34mf1:\u001b[0m 0.9737 ± 0.0068, \u001b[34mmcc:\u001b[0m 0.4847 ± 0.0898, \u001b[34mf1 micro:\u001b[0m 0.4461 ± 0.1002,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1149, \u001b[34mclf_grad:\u001b[0m 0.0998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.87it/s]6.88it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9775669667869806 0.005161757301840296 0.9748568012757683 0.00634802017282843 0.507801296949032 0.09642820857799296\n",
      "23/12/06 18:28:09, ep  5, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9776 ± 0.0052, \u001b[34mf1:\u001b[0m 0.9749 ± 0.0063, \u001b[34mmcc:\u001b[0m 0.5078 ± 0.0964, \u001b[34mf1 micro:\u001b[0m 0.4784 ± 0.1038,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1279, \u001b[34mclf_grad:\u001b[0m 0.0997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m23/12/06 18:28:09, Attempt to save cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/5\u001b[0m\n",
      "\u001b[35m23/12/06 18:28:09, Saved cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/5\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.71it/s]7.64it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9773995634168386 0.006570383200879331 0.9729551166226778 0.00860186803512064 0.47519437354974675 0.09075064762394232\n",
      "23/12/06 18:28:30, ep  6, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9774 ± 0.0066, \u001b[34mf1:\u001b[0m 0.9730 ± 0.0086, \u001b[34mmcc:\u001b[0m 0.4752 ± 0.0908, \u001b[34mf1 micro:\u001b[0m 0.4325 ± 0.1013,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1415, \u001b[34mclf_grad:\u001b[0m 0.1017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 19.35it/s]6.69it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9779296945780516 0.0063966916839733835 0.973965467784162 0.008228119696992244 0.49336349394641543 0.09798029397413488\n",
      "23/12/06 18:28:54, ep  7, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9779 ± 0.0064, \u001b[34mf1:\u001b[0m 0.9740 ± 0.0082, \u001b[34mmcc:\u001b[0m 0.4934 ± 0.0980, \u001b[34mf1 micro:\u001b[0m 0.4570 ± 0.1094,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1508, \u001b[34mclf_grad:\u001b[0m 0.1066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 19.00it/s]7.85it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977399555966258 0.0062557353879579515 0.973985575855568 0.007686374240762557 0.49506861100825234 0.10291729203907361\n",
      "23/12/06 18:29:16, ep  8, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9774 ± 0.0063, \u001b[34mf1:\u001b[0m 0.9740 ± 0.0077, \u001b[34mmcc:\u001b[0m 0.4951 ± 0.1029, \u001b[34mf1 micro:\u001b[0m 0.4688 ± 0.1129,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1572, \u001b[34mclf_grad:\u001b[0m 0.1089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 16.48it/s]6.99it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9776506815105677 0.006701956699265997 0.9739437503646753 0.00841793817946136 0.4956505994889814 0.10624296712866835\n",
      "23/12/06 18:29:39, ep  9, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9777 ± 0.0067, \u001b[34mf1:\u001b[0m 0.9739 ± 0.0084, \u001b[34mmcc:\u001b[0m 0.4957 ± 0.1062, \u001b[34mf1 micro:\u001b[0m 0.4662 ± 0.1146,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1638, \u001b[34mclf_grad:\u001b[0m 0.1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 18.27it/s]7.04it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9776227753609419 0.006092315641565041 0.9747218957292946 0.007701162929675861 0.5105214415346788 0.09506578666910709\n",
      "23/12/06 18:30:02, ep 10, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9776 ± 0.0061, \u001b[34mf1:\u001b[0m 0.9747 ± 0.0077, \u001b[34mmcc:\u001b[0m 0.5105 ± 0.0951, \u001b[34mf1 micro:\u001b[0m 0.4848 ± 0.1039,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1623, \u001b[34mclf_grad:\u001b[0m 0.1081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m23/12/06 18:30:02, Attempt to save cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/10\u001b[0m\n",
      "\u001b[35m23/12/06 18:30:02, Saved cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/10\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 22.54it/s]7.02it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9772042408585548 0.0065382543585169916 0.9743336596223788 0.008305386632777427 0.5031150722330291 0.09502359162897324\n",
      "23/12/06 18:30:25, ep 11, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9772 ± 0.0065, \u001b[34mf1:\u001b[0m 0.9743 ± 0.0083, \u001b[34mmcc:\u001b[0m 0.5031 ± 0.0950, \u001b[34mf1 micro:\u001b[0m 0.4743 ± 0.1055,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1617, \u001b[34mclf_grad:\u001b[0m 0.1091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.93it/s]7.07it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9761718772351742 0.006608192894195396 0.9747384425411063 0.007666926452982785 0.5135870944008513 0.1010375938840236\n",
      "23/12/06 18:30:47, ep 12, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9762 ± 0.0066, \u001b[34mf1:\u001b[0m 0.9747 ± 0.0077, \u001b[34mmcc:\u001b[0m 0.5136 ± 0.1010, \u001b[34mf1 micro:\u001b[0m 0.4975 ± 0.1023,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1591, \u001b[34mclf_grad:\u001b[0m 0.1018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m23/12/06 18:30:47, Attempt to save cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/12\u001b[0m\n",
      "\u001b[35m23/12/06 18:30:47, Saved cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/12\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 20.25it/s]7.18it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763671923428774 0.0067814008392737294 0.9739937521318889 0.008018936561508087 0.49359904017387596 0.09163766003382175\n",
      "23/12/06 18:31:11, ep 13, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9764 ± 0.0068, \u001b[34mf1:\u001b[0m 0.9740 ± 0.0080, \u001b[34mmcc:\u001b[0m 0.4936 ± 0.0916, \u001b[34mf1 micro:\u001b[0m 0.4718 ± 0.0948,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1697, \u001b[34mclf_grad:\u001b[0m 0.1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.76it/s]5.76it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756975509226322 0.006192879530811608 0.9743980286287799 0.007169164923078567 0.509153506855511 0.09152496732606163\n",
      "23/12/06 18:31:34, ep 14, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9757 ± 0.0062, \u001b[34mf1:\u001b[0m 0.9744 ± 0.0072, \u001b[34mmcc:\u001b[0m 0.5092 ± 0.0915, \u001b[34mf1 micro:\u001b[0m 0.4893 ± 0.0965,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1688, \u001b[34mclf_grad:\u001b[0m 0.1219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 17.29it/s]9.47it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756696466356516 0.00617380015822628 0.9745722945758711 0.007007133334605766 0.5119974716549991 0.07685955024565447\n",
      "23/12/06 18:31:57, ep 15, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9757 ± 0.0062, \u001b[34mf1:\u001b[0m 0.9746 ± 0.0070, \u001b[34mmcc:\u001b[0m 0.5120 ± 0.0769, \u001b[34mf1 micro:\u001b[0m 0.4871 ± 0.0803,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1690, \u001b[34mclf_grad:\u001b[0m 0.1313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.29it/s]7.80it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756975453346968 0.006429712715748347 0.9753374108104597 0.006911502438781174 0.5442374246225116 0.09108255810328089\n",
      "23/12/06 18:32:20, ep 16, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9757 ± 0.0064, \u001b[34mf1:\u001b[0m 0.9753 ± 0.0069, \u001b[34mmcc:\u001b[0m 0.5442 ± 0.0911, \u001b[34mf1 micro:\u001b[0m 0.5256 ± 0.0994,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1606, \u001b[34mclf_grad:\u001b[0m 0.1288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m23/12/06 18:32:20, Attempt to save cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/16\u001b[0m\n",
      "\u001b[35m23/12/06 18:32:20, Saved cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/16\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 19.29it/s]6.18it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9741908553987741 0.0057620361352376205 0.9743206682907524 0.0060139958865119405 0.529816524306306 0.08254728785832569\n",
      "23/12/06 18:32:44, ep 17, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9742 ± 0.0058, \u001b[34mf1:\u001b[0m 0.9743 ± 0.0060, \u001b[34mmcc:\u001b[0m 0.5298 ± 0.0825, \u001b[34mf1 micro:\u001b[0m 0.5095 ± 0.0905,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1551, \u001b[34mclf_grad:\u001b[0m 0.1211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 20.19it/s]6.65it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9731026794761419 0.006646955252707948 0.973880122580267 0.006753947151760082 0.5344976156313386 0.0919047234635493\n",
      "23/12/06 18:33:07, ep 18, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9731 ± 0.0066, \u001b[34mf1:\u001b[0m 0.9739 ± 0.0068, \u001b[34mmcc:\u001b[0m 0.5345 ± 0.0919, \u001b[34mf1 micro:\u001b[0m 0.5142 ± 0.1045,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1470, \u001b[34mclf_grad:\u001b[0m 0.1094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 19.77it/s]8.73it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9748046901077032 0.005438989355992289 0.9753136068375543 0.005752718823685731 0.5556165717241273 0.09656801254374908\n",
      "23/12/06 18:33:30, ep 19, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9748 ± 0.0054, \u001b[34mf1:\u001b[0m 0.9753 ± 0.0058, \u001b[34mmcc:\u001b[0m 0.5556 ± 0.0966, \u001b[34mf1 micro:\u001b[0m 0.5389 ± 0.1058,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1455, \u001b[34mclf_grad:\u001b[0m 0.1041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m23/12/06 18:33:30, Attempt to save cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/19\u001b[0m\n",
      "\u001b[35m23/12/06 18:33:30, Saved cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/19\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 19.63it/s]7.08it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9750837124884129 0.006978293767178298 0.9751199864363483 0.007442924845762097 0.555868423194328 0.11091076598598433\n",
      "23/12/06 18:33:53, ep 20, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9751 ± 0.0070, \u001b[34mf1:\u001b[0m 0.9751 ± 0.0074, \u001b[34mmcc:\u001b[0m 0.5559 ± 0.1109, \u001b[34mf1 micro:\u001b[0m 0.5410 ± 0.1199,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1443, \u001b[34mclf_grad:\u001b[0m 0.1027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m23/12/06 18:33:53, Attempt to save cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/20\u001b[0m\n",
      "\u001b[35m23/12/06 18:33:53, Saved cur best model to /home/hyuns6100/Mental-Heatlh-Care/Result/20\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 18.56it/s]6.91it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746093768626451 0.005508188097351017 0.9748158258218429 0.0060825738994552585 0.5430431229934503 0.07870469978764531\n",
      "23/12/06 18:34:16, ep 21, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9746 ± 0.0055, \u001b[34mf1:\u001b[0m 0.9748 ± 0.0061, \u001b[34mmcc:\u001b[0m 0.5430 ± 0.0787, \u001b[34mf1 micro:\u001b[0m 0.5235 ± 0.0891,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1444, \u001b[34mclf_grad:\u001b[0m 0.0978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 20.01it/s]7.80it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9762276802212 0.005257396522818682 0.9750673467051068 0.00639069284578478 0.5301149414028219 0.09922262407395413\n",
      "23/12/06 18:34:40, ep 22, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9762 ± 0.0053, \u001b[34mf1:\u001b[0m 0.9751 ± 0.0064, \u001b[34mmcc:\u001b[0m 0.5301 ± 0.0992, \u001b[34mf1 micro:\u001b[0m 0.5157 ± 0.1068,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1407, \u001b[34mclf_grad:\u001b[0m 0.0953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 22.09it/s]4.95it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746930859982967 0.005029069433947089 0.9746497885036904 0.0056912694867659215 0.5390246045879209 0.10073673890021506\n",
      "23/12/06 18:35:03, ep 23, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9747 ± 0.0050, \u001b[34mf1:\u001b[0m 0.9746 ± 0.0057, \u001b[34mmcc:\u001b[0m 0.5390 ± 0.1007, \u001b[34mf1 micro:\u001b[0m 0.5246 ± 0.1093,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1433, \u001b[34mclf_grad:\u001b[0m 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 20.06it/s]4.81it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9760323688387871 0.0063532236174185875 0.9751145114505748 0.007203486936075662 0.5373399247378805 0.10007625878147375\n",
      "23/12/06 18:35:27, ep 24, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9760 ± 0.0064, \u001b[34mf1:\u001b[0m 0.9751 ± 0.0072, \u001b[34mmcc:\u001b[0m 0.5373 ± 0.1001, \u001b[34mf1 micro:\u001b[0m 0.5188 ± 0.1109,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1424, \u001b[34mclf_grad:\u001b[0m 0.0979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 19.57it/s]7.30it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9755859449505806 0.0057846936153213594 0.9748577269135225 0.006792035997139825 0.5335551566305137 0.09044981228650588\n",
      "23/12/06 18:35:50, ep 25, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9756 ± 0.0058, \u001b[34mf1:\u001b[0m 0.9749 ± 0.0068, \u001b[34mmcc:\u001b[0m 0.5336 ± 0.0904, \u001b[34mf1 micro:\u001b[0m 0.5233 ± 0.0974,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1382, \u001b[34mclf_grad:\u001b[0m 0.0923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 18.79it/s]7.12it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763113874942064 0.005575829470871002 0.9758840884033482 0.00606917669216701 0.5482557445011781 0.10467543848351427\n",
      "23/12/06 18:36:14, ep 26, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9763 ± 0.0056, \u001b[34mf1:\u001b[0m 0.9759 ± 0.0061, \u001b[34mmcc:\u001b[0m 0.5483 ± 0.1047, \u001b[34mf1 micro:\u001b[0m 0.5323 ± 0.1087,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1381, \u001b[34mclf_grad:\u001b[0m 0.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 20.00it/s]5.39it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9754464346915483 0.005182230438994357 0.9754448154192177 0.005868707595749511 0.5544827539520831 0.09872154668204611\n",
      "23/12/06 18:36:37, ep 27, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9754 ± 0.0052, \u001b[34mf1:\u001b[0m 0.9754 ± 0.0059, \u001b[34mmcc:\u001b[0m 0.5545 ± 0.0987, \u001b[34mf1 micro:\u001b[0m 0.5398 ± 0.1084,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1345, \u001b[34mclf_grad:\u001b[0m 0.0918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.55it/s]6.84it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9744698666036129 0.005610901040937477 0.9748850076340039 0.005972139936102184 0.5502738610717739 0.08548392070428114\n",
      "23/12/06 18:37:00, ep 28, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9745 ± 0.0056, \u001b[34mf1:\u001b[0m 0.9749 ± 0.0060, \u001b[34mmcc:\u001b[0m 0.5503 ± 0.0855, \u001b[34mf1 micro:\u001b[0m 0.5303 ± 0.0966,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1403, \u001b[34mclf_grad:\u001b[0m 0.0982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 16.83it/s]5.95it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9755859430879354 0.005776079852190839 0.9745241933692568 0.006840088170856422 0.5211108995249822 0.09948229166855717\n",
      "23/12/06 18:37:23, ep 29, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9756 ± 0.0058, \u001b[34mf1:\u001b[0m 0.9745 ± 0.0068, \u001b[34mmcc:\u001b[0m 0.5211 ± 0.0995, \u001b[34mf1 micro:\u001b[0m 0.5065 ± 0.1107,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1366, \u001b[34mclf_grad:\u001b[0m 0.0984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 16.86it/s]6.74it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9734654016792774 0.0055095271856676785 0.9738121693238232 0.006199318639912829 0.5329809063239597 0.08275367612876919\n",
      "23/12/06 18:37:47, ep 30, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9735 ± 0.0055, \u001b[34mf1:\u001b[0m 0.9738 ± 0.0062, \u001b[34mmcc:\u001b[0m 0.5330 ± 0.0828, \u001b[34mf1 micro:\u001b[0m 0.5151 ± 0.0911,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1320, \u001b[34mclf_grad:\u001b[0m 0.0916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 20.12it/s]7.84it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9729073699563742 0.0068056900286164014 0.9728431486749618 0.007734928704607927 0.5109303627292878 0.0949874441170185\n",
      "23/12/06 18:38:09, ep 31, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9729 ± 0.0068, \u001b[34mf1:\u001b[0m 0.9728 ± 0.0077, \u001b[34mmcc:\u001b[0m 0.5109 ± 0.0950, \u001b[34mf1 micro:\u001b[0m 0.4965 ± 0.1001,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1356, \u001b[34mclf_grad:\u001b[0m 0.0971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 22.21it/s]8.72it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9767578113824129 0.005666671880597118 0.9754575998421774 0.006751572446996948 0.5312787656571556 0.08607286787973752\n",
      "23/12/06 18:38:31, ep 32, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9768 ± 0.0057, \u001b[34mf1:\u001b[0m 0.9755 ± 0.0068, \u001b[34mmcc:\u001b[0m 0.5313 ± 0.0861, \u001b[34mf1 micro:\u001b[0m 0.5111 ± 0.0955,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1303, \u001b[34mclf_grad:\u001b[0m 0.0971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.90it/s]7.95it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976283485069871 0.005490067523459321 0.9752808567044919 0.006506435242742771 0.5289561861014835 0.07928053241550868\n",
      "23/12/06 18:38:54, ep 33, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9763 ± 0.0055, \u001b[34mf1:\u001b[0m 0.9753 ± 0.0065, \u001b[34mmcc:\u001b[0m 0.5290 ± 0.0793, \u001b[34mf1 micro:\u001b[0m 0.5124 ± 0.0875,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1326, \u001b[34mclf_grad:\u001b[0m 0.1012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 19.61it/s]7.71it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756417442113161 0.005795990535352611 0.9747563752238537 0.006883163015858237 0.5174425135152636 0.0820362852919902\n",
      "23/12/06 18:39:16, ep 34, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9756 ± 0.0058, \u001b[34mf1:\u001b[0m 0.9748 ± 0.0069, \u001b[34mmcc:\u001b[0m 0.5174 ± 0.0820, \u001b[34mf1 micro:\u001b[0m 0.4912 ± 0.0928,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1289, \u001b[34mclf_grad:\u001b[0m 0.0971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.55it/s]7.69it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9766183104366064 0.006642036100523427 0.9738588783007525 0.008148733315871869 0.4903623938641161 0.07978604383744642\n",
      "23/12/06 18:39:38, ep 35, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9766 ± 0.0066, \u001b[34mf1:\u001b[0m 0.9739 ± 0.0081, \u001b[34mmcc:\u001b[0m 0.4904 ± 0.0798, \u001b[34mf1 micro:\u001b[0m 0.4548 ± 0.0861,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1293, \u001b[34mclf_grad:\u001b[0m 0.0977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 23.42it/s]7.62it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9775948710739613 0.005890326339787389 0.9751002691447206 0.007366846411222162 0.5162757860117612 0.08898311624040531\n",
      "23/12/06 18:40:00, ep 36, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9776 ± 0.0059, \u001b[34mf1:\u001b[0m 0.9751 ± 0.0074, \u001b[34mmcc:\u001b[0m 0.5163 ± 0.0890, \u001b[34mf1 micro:\u001b[0m 0.4910 ± 0.0975,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1282, \u001b[34mclf_grad:\u001b[0m 0.0974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 19.72it/s]7.98it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9761718790978193 0.005936993256153652 0.9742882751864121 0.007270123461611761 0.498204995997757 0.0864751444626322\n",
      "23/12/06 18:40:22, ep 37, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9762 ± 0.0059, \u001b[34mf1:\u001b[0m 0.9743 ± 0.0073, \u001b[34mmcc:\u001b[0m 0.4982 ± 0.0865, \u001b[34mf1 micro:\u001b[0m 0.4760 ± 0.0989,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1283, \u001b[34mclf_grad:\u001b[0m 0.1059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 22.99it/s]6.72it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9755580369383097 0.006256975192994532 0.9735975635046148 0.0074341860792987 0.4838903637337089 0.10137756508557653\n",
      "23/12/06 18:40:45, ep 38, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9756 ± 0.0063, \u001b[34mf1:\u001b[0m 0.9736 ± 0.0074, \u001b[34mmcc:\u001b[0m 0.4839 ± 0.1014, \u001b[34mf1 micro:\u001b[0m 0.4624 ± 0.1103,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1276, \u001b[34mclf_grad:\u001b[0m 0.1086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 22.63it/s]8.49it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9771205428987741 0.005676631027626605 0.9753447853545362 0.006877808760017291 0.5264766890313131 0.07955383484016762\n",
      "23/12/06 18:41:07, ep 39, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9771 ± 0.0057, \u001b[34mf1:\u001b[0m 0.9753 ± 0.0069, \u001b[34mmcc:\u001b[0m 0.5265 ± 0.0796, \u001b[34mf1 micro:\u001b[0m 0.5076 ± 0.0863,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1212, \u001b[34mclf_grad:\u001b[0m 0.1007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 20.07it/s]7.57it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9764229878783226 0.006273314098769167 0.9744503013846995 0.0076483319502669645 0.5114783594066977 0.0860808804202352\n",
      "23/12/06 18:41:29, ep 40, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9764 ± 0.0063, \u001b[34mf1:\u001b[0m 0.9745 ± 0.0076, \u001b[34mmcc:\u001b[0m 0.5115 ± 0.0861, \u001b[34mf1 micro:\u001b[0m 0.4932 ± 0.0945,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1160, \u001b[34mclf_grad:\u001b[0m 0.0931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 17.15it/s]6.78it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9765345957130194 0.005825999899047127 0.9744920307749925 0.006961920275051609 0.5052030376278522 0.0872983667552736\n",
      "23/12/06 18:41:52, ep 41, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9765 ± 0.0058, \u001b[34mf1:\u001b[0m 0.9745 ± 0.0070, \u001b[34mmcc:\u001b[0m 0.5052 ± 0.0873, \u001b[34mf1 micro:\u001b[0m 0.4860 ± 0.0917,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1177, \u001b[34mclf_grad:\u001b[0m 0.0969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 23.27it/s]7.47it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9759765677154064 0.006551100689700259 0.9737830101572678 0.00790782612817721 0.4901986344019371 0.08458719681717364\n",
      "23/12/06 18:42:14, ep 42, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9760 ± 0.0066, \u001b[34mf1:\u001b[0m 0.9738 ± 0.0079, \u001b[34mmcc:\u001b[0m 0.4902 ± 0.0846, \u001b[34mf1 micro:\u001b[0m 0.4704 ± 0.0930,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1158, \u001b[34mclf_grad:\u001b[0m 0.0944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 22.38it/s]6.90it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9758928623050451 0.006141444351919954 0.9742559105430537 0.007322479085612812 0.5048093097581048 0.08724101852869341\n",
      "23/12/06 18:42:36, ep 43, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9759 ± 0.0061, \u001b[34mf1:\u001b[0m 0.9743 ± 0.0073, \u001b[34mmcc:\u001b[0m 0.5048 ± 0.0872, \u001b[34mf1 micro:\u001b[0m 0.4818 ± 0.0963,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1241, \u001b[34mclf_grad:\u001b[0m 0.1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 18.21it/s]5.62it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9762834832072258 0.006269653080216177 0.9747850741671298 0.00729745162880773 0.5250822169062805 0.07828861183193497\n",
      "23/12/06 18:42:59, ep 44, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9763 ± 0.0063, \u001b[34mf1:\u001b[0m 0.9748 ± 0.0073, \u001b[34mmcc:\u001b[0m 0.5251 ± 0.0783, \u001b[34mf1 micro:\u001b[0m 0.5079 ± 0.0810,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1193, \u001b[34mclf_grad:\u001b[0m 0.0974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 20.28it/s]7.01it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9761160705238581 0.006344942093911509 0.9745684577143958 0.007423712515041549 0.5117290758990223 0.07915807402113076\n",
      "23/12/06 18:43:22, ep 45, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9761 ± 0.0063, \u001b[34mf1:\u001b[0m 0.9746 ± 0.0074, \u001b[34mmcc:\u001b[0m 0.5117 ± 0.0792, \u001b[34mf1 micro:\u001b[0m 0.4976 ± 0.0854,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1148, \u001b[34mclf_grad:\u001b[0m 0.0940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 20.31it/s]6.70it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9750000052154064 0.00626989764906004 0.9746914929942638 0.006676024353273723 0.5301817924235681 0.07984802733968373\n",
      "23/12/06 18:43:45, ep 46, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9750 ± 0.0063, \u001b[34mf1:\u001b[0m 0.9747 ± 0.0067, \u001b[34mmcc:\u001b[0m 0.5302 ± 0.0798, \u001b[34mf1 micro:\u001b[0m 0.5130 ± 0.0870,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1235, \u001b[34mclf_grad:\u001b[0m 0.0978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 18.14it/s]7.51it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9753627274185419 0.005162286617750854 0.9741630672326571 0.005989965843704185 0.5038489280093315 0.07439019039183095\n",
      "23/12/06 18:44:08, ep 47, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9754 ± 0.0052, \u001b[34mf1:\u001b[0m 0.9742 ± 0.0060, \u001b[34mmcc:\u001b[0m 0.5038 ± 0.0744, \u001b[34mf1 micro:\u001b[0m 0.4880 ± 0.0812,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1182, \u001b[34mclf_grad:\u001b[0m 0.0979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.31it/s]6.52it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9761160761117935 0.006169775011174002 0.9746053096873467 0.0072470378336012406 0.5134200106921656 0.06608746590540779\n",
      "23/12/06 18:44:31, ep 48, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9761 ± 0.0062, \u001b[34mf1:\u001b[0m 0.9746 ± 0.0072, \u001b[34mmcc:\u001b[0m 0.5134 ± 0.0661, \u001b[34mf1 micro:\u001b[0m 0.4933 ± 0.0735,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1177, \u001b[34mclf_grad:\u001b[0m 0.0985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.52it/s]6.94it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763671960681677 0.006469304519178751 0.9736552840324121 0.008010762498960175 0.48989775739468316 0.09411799069483878\n",
      "23/12/06 18:44:53, ep 49, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9764 ± 0.0065, \u001b[34mf1:\u001b[0m 0.9737 ± 0.0080, \u001b[34mmcc:\u001b[0m 0.4899 ± 0.0941, \u001b[34mf1 micro:\u001b[0m 0.4669 ± 0.1028,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1173, \u001b[34mclf_grad:\u001b[0m 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 19.42it/s]7.49it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9752232171595097 0.007118401651123753 0.9733954375035826 0.00828162161878278 0.48814614895031555 0.079989059508086\n",
      "23/12/06 18:45:16, ep 50, \u001b[36mval  \u001b[0m \u001b[34macc:\u001b[0m 0.9752 ± 0.0071, \u001b[34mf1:\u001b[0m 0.9734 ± 0.0083, \u001b[34mmcc:\u001b[0m 0.4881 ± 0.0800, \u001b[34mf1 micro:\u001b[0m 0.4667 ± 0.0894,\u001b[36mtrain stats\u001b[0m \u001b[34mebd_grad:\u001b[0m 0.1176, \u001b[34mclf_grad:\u001b[0m 0.1038\n",
      "23/12/06 18:45:16, End of training. Restore the best weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m23/12/06 18:45:16, Save best model to /home/hyuns6100/Mental-Heatlh-Care/Result/result_text/best\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train(train_data, val_data, model, args, loader=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on val\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 21.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/06 18:45:31, \u001b[34macc mean\u001b[0m  0.9751 (\u001b[34mstd\u001b[0m  0.0070), \u001b[34mf1 mean\u001b[0m  0.9751 (\u001b[34mstd\u001b[0m  0.0074), \u001b[34mmcc mean\u001b[0m  0.5559 (\u001b[34mstd\u001b[0m  0.1109), \u001b[34mf1 micro mean\u001b[0m  0.5410 (\u001b[34mstd\u001b[0m  0.1199)\n",
      "cnn & mlp &  0.9751 \\tiny $\\pm  0.0070$ &  0.9751 \\tiny $\\pm  0.0074$ &  0.5559 \\tiny $\\pm  0.1109$ &  0.5410 \\tiny $\\pm  0.1199$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_acc, val_std, _, _, _, _, val_f1_micro, val_f1_micro_std = test(val_data, model, args, verbose=True, target='val', loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mtest_data\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on test\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 23.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/06 18:45:45, \u001b[34macc mean\u001b[0m  0.9637 (\u001b[34mstd\u001b[0m  0.0112), \u001b[34mf1 mean\u001b[0m  0.9613 (\u001b[34mstd\u001b[0m  0.0132), \u001b[34mmcc mean\u001b[0m  0.4940 (\u001b[34mstd\u001b[0m  0.0956), \u001b[34mf1 micro mean\u001b[0m  0.4837 (\u001b[34mstd\u001b[0m  0.0988)\n",
      "cnn & mlp &  0.9637 \\tiny $\\pm  0.0112$ &  0.9613 \\tiny $\\pm  0.0132$ &  0.4940 \\tiny $\\pm  0.0956$ &  0.4837 \\tiny $\\pm  0.0988$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print( colored('test_data', 'green') )\n",
    "\n",
    "test_acc, test_std, _, _, _, _, test_f1_micro, test_f1_micro_std, = test(test_data, model, args, target='test', loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.best_result_path:\n",
    "    directory = args.best_result_path[:args.best_result_path.rfind(\"/\")]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    result = {\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_std\": test_std,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_std\": val_std,\n",
    "        \"test_f1_micro\": test_f1_micro,\n",
    "        \"test_f1_micro_std\": test_f1_micro_std,\n",
    "        \"val_f1_micro\": val_f1_micro,\n",
    "        \"val_f1_micro_std\": val_f1_micro_std\n",
    "    }\n",
    "\n",
    "    for attr, value in sorted(args.__dict__.items()):\n",
    "        result[attr] = value\n",
    "\n",
    "    with open(args.best_result_path, \"wb\") as f:\n",
    "        pickle.dump(result, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': 0.9636718761175871, 'test_std': 0.011181758962263125, 'val_acc': 0.9750837124884129, 'val_std': 0.006978293767178298, 'test_f1_micro': 0.4836933431523283, 'test_f1_micro_std': 0.09875537229629391, 'val_f1_micro': 0.5409813460077257, 'val_f1_micro_std': 0.11992616569879222, 'authors': False, 'batch_size': 32, 'best_result_path': '/home/hyuns6100/Mental-Heatlh-Care/Result/best_results.pkl', 'classifier': 'mlp', 'cnn_filter_sizes': [3, 4, 5], 'cnn_num_filters': 100, 'context_size': 35, 'convmode': 'seq', 'data_path': '/home/hyuns6100/Mental-Heatlh-Care/data/dailydialog_conv35seq_splits.json', 'dropout': 0.1, 'embedding': 'cnn', 'finetune_ebd': False, 'labels': [1, 2, 3, 4, 5, 6], 'lr': 0.001, 'maxtokens': 30, 'mlp_hidden': [300, 300], 'n_classes': 7, 'n_test_class': 7, 'n_train_class': 7, 'n_val_class': 7, 'patience': 30, 'patience_metric': 'f1_micro', 'result_path': '/home/hyuns6100/Mental-Heatlh-Care/Result/', 'result_text_path': '/home/hyuns6100/Mental-Heatlh-Care/Result/result_text/', 'save': True, 'seed': 330, 'train_epochs': 1000, 'word_vector': 'wiki-news-300d-1M.vec', 'wv_path': '/home/hyuns6100/Mental-Heatlh-Care/data/'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_torch(audio_path,checkpoint_path):\n",
    "#     torch_model = resnet34().cuda()\n",
    "#     load_model = torch.load(checkpoint_path)\n",
    "#     torch_model.load_state_dict(load_model)\n",
    "#     torch_model.eval()\n",
    "    \n",
    "\n",
    "#     paths = glob.glob(audio_path+\"/*\")\n",
    "#     count = 0\n",
    "#     with torch.no_grad():\n",
    "#         count = 0\n",
    "#         for input_path in tqdm(paths):\n",
    "            \n",
    "#             label_ = int(input_path.split('-')[2]) - 1\n",
    "#             input_data = preprocess_audio(input_path)\n",
    "#             input_data = np.transpose(input_data, axes=[2, 0, 1])\n",
    "#             input_data = np.expand_dims(input_data, axis=0)\n",
    "#             input_data = torch.FloatTensor(input_data).cuda()\n",
    "#             outputs = torch_model(input_data)\n",
    "                \n",
    "#             pred = torch.argmax(outputs.cpu())\n",
    "\n",
    "#             if label_ == pred:\n",
    "#                 count += 1\n",
    "            \n",
    "#     print(\"=======================\")\n",
    "#     print(\">> Pytorch result\")\n",
    "#     print(f\"Acc: {count/len(paths)}\")\n",
    "#     print(\"=======================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onnx 변환 전 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPseq(\n",
       "  (mlp): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (out): Linear(in_features=300, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#best_path = \"/home/hyuns6100/Mental-Heatlh-Care/Result/\"\n",
    "best_path = \"/home/hyuns6100/Mental-Heatlh-Care/Result/result_text/best\"\n",
    "\n",
    "# 모델 state path\n",
    "ebd_model_path = best_path + '.ebd'\n",
    "clf_model_path = best_path + '.clf'\n",
    "\n",
    "ebd_model_params = torch.load(ebd_model_path, map_location=device)\n",
    "clf_model_params = torch.load(clf_model_path, map_location=device)\n",
    "\n",
    "# 모델 정의 및 불러온 파라미터 설정\n",
    "try_model = {}\n",
    "wordebd = WORDEBD(vocab, finetune_ebd=False)\n",
    "ebd = CNNseq(wordebd, args).to(device)\n",
    "try_model['ebd'] = ebd\n",
    "\n",
    "clf = MLPseq(try_model[\"ebd\"].ebd_dim, args).to(device)\n",
    "try_model['clf'] = clf\n",
    "\n",
    "try_model['ebd'].load_state_dict(ebd_model_params)\n",
    "try_model['clf'].load_state_dict(clf_model_params)\n",
    "\n",
    "try_model['ebd'].eval()\n",
    "try_model['clf'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTesting regular on test\u001b[0m: 100%|██████████| 32/32 [00:01<00:00, 25.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/06 18:55:06, \u001b[34macc mean\u001b[0m  0.9637 (\u001b[34mstd\u001b[0m  0.0112), \u001b[34mf1 mean\u001b[0m  0.9613 (\u001b[34mstd\u001b[0m  0.0132), \u001b[34mmcc mean\u001b[0m  0.4940 (\u001b[34mstd\u001b[0m  0.0956), \u001b[34mf1 micro mean\u001b[0m  0.4837 (\u001b[34mstd\u001b[0m  0.0988)\n",
      "cnn & mlp &  0.9637 \\tiny $\\pm  0.0112$ &  0.9613 \\tiny $\\pm  0.0132$ &  0.4940 \\tiny $\\pm  0.0956$ &  0.4837 \\tiny $\\pm  0.0988$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_std, _, _, _, _, test_f1_micro, test_f1_micro_std, = test(test_data, try_model, args, target='test', loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# #best_path = \"/home/hyuns6100/Mental-Heatlh-Care/Result/\"\n",
    "# best_path = \"/home/hyuns6100/Mental-Heatlh-Care/Result/result_text/best\"\n",
    "\n",
    "# # 모델 state path\n",
    "# ebd_model_path = best_path + '.ebd'\n",
    "# clf_model_path = best_path + '.clf'\n",
    "\n",
    "# ebd_model_params = torch.load(ebd_model_path, map_location=device)\n",
    "# clf_model_params = torch.load(clf_model_path, map_location=device)\n",
    "\n",
    "# # 모델 정의 및 불러온 파라미터 설정\n",
    "# model = {}\n",
    "# wordebd = WORDEBD(vocab, finetune_ebd=False)\n",
    "# ebd = CNNseq(wordebd, args).to(device)\n",
    "# model['ebd'] = ebd\n",
    "\n",
    "# clf = MLPseq(model[\"ebd\"].ebd_dim, args).to(device)\n",
    "# model['clf'] = clf\n",
    "\n",
    "# model['ebd'].load_state_dict(ebd_model_params)\n",
    "# model['clf'].load_state_dict(clf_model_params)\n",
    "\n",
    "# model['ebd'].eval()\n",
    "# model['clf'].eval()\n",
    "\n",
    "# calibration data \n",
    "batch = test_loader.dataset[0]\n",
    "batch['text'] = torch.tensor(batch['text']).unsqueeze(0).to(device)\n",
    "\n",
    "XS = try_model['ebd'](batch)\n",
    "# XS_np = XS.detach().cpu().numpy()\n",
    "\n",
    "# # save to .npy\n",
    "# np.save('/home/hyuns6100/Mental-Heatlh-Care/onnx/emotion_calib_1', XS_np)\n",
    "\n",
    "# convert to onnx\n",
    "torch.onnx.export(try_model['clf'], XS, \"/home/hyuns6100/Mental-Heatlh-Care/onnx/emo_clf_onnx.onxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_np_data = np.load('/home/hyuns6100/Mental-Heatlh-Care/onnx/emotion_test_data.npy')\n",
    "test_np_label = np.load('/home/hyuns6100/Mental-Heatlh-Care/onnx/emotion_test_label.npy')\n",
    "\n",
    "batch = torch.tensor(test_np_data).to(device)\n",
    "batch_label = torch.tensor(test_np_label).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1465756893157959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9594571590423584, 0.9396051476148122, 0.0, 0.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_one 함수\n",
    "s = time.time()\n",
    "out_XS = try_model['clf'](batch, YS=None)\n",
    "print(time.time() - s)\n",
    "\n",
    "output = out_XS.view(-1, args.n_classes)  # new shape: [32*35, 7] (batch_size=32)\n",
    "target = batch_label.view(-1)\n",
    "\n",
    "acc = BASE.compute_acc(output, target)\n",
    "f1 = BASE.compute_f1(output, target)\n",
    "mcc = BASE.compute_mcc(output, target)\n",
    "micro_f1_noneutral = BASE.compute_f1_micro_noneutral(output, target, labels=args['labels'])\n",
    "\n",
    "acc, f1, mcc, micro_f1_noneutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9595, device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.argmax(output, dim=1) == target).float().sum() / len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to npy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPseq(\n",
       "  (mlp): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (out): Linear(in_features=300, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_model['ebd'].eval()\n",
    "try_model['clf'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = test_loader.dataset[:]\n",
    "batch['text'] = torch.tensor(batch['text']).to(device)\n",
    "\n",
    "YS = torch.tensor(batch['label']).to(device)\n",
    "XS = try_model['ebd'](batch)\n",
    "XS_np = XS.detach().cpu().numpy()\n",
    "YS_np = YS.detach().cpu().numpy()\n",
    "\n",
    "# save to .npy\n",
    "np.save('/home/hyuns6100/Mental-Heatlh-Care/onnx/emotion_test_data', XS_np)\n",
    "np.save('/home/hyuns6100/Mental-Heatlh-Care/onnx/emotion_test_label', YS_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성능 확인 마지막!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPseq(\n",
       "  (mlp): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (out): Linear(in_features=300, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#best_path = \"/home/hyuns6100/Mental-Heatlh-Care/Result/\"\n",
    "best_path = \"/home/hyuns6100/Mental-Heatlh-Care/Result/result_text/best\"\n",
    "\n",
    "# 모델 state path\n",
    "ebd_model_path = best_path + '.ebd'\n",
    "clf_model_path = best_path + '.clf'\n",
    "\n",
    "ebd_model_params = torch.load(ebd_model_path, map_location=device)\n",
    "clf_model_params = torch.load(clf_model_path, map_location=device)\n",
    "\n",
    "# 모델 정의 및 불러온 파라미터 설정\n",
    "try_model = {}\n",
    "wordebd = WORDEBD(vocab, finetune_ebd=False)\n",
    "ebd = CNNseq(wordebd, args).to(device)\n",
    "try_model['ebd'] = ebd\n",
    "\n",
    "clf = MLPseq(try_model[\"ebd\"].ebd_dim, args).to(device)\n",
    "try_model['clf'] = clf\n",
    "\n",
    "try_model['ebd'].load_state_dict(ebd_model_params)\n",
    "try_model['clf'].load_state_dict(clf_model_params)\n",
    "\n",
    "try_model['ebd'].eval()\n",
    "try_model['clf'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9634857177734375,\n",
       " 0.9612830286849585,\n",
       " 0.4899312517676465,\n",
       " 0.4828393135725429)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try_model: best 모델 파라미터 load \n",
    "# XS: test_loader 에 저장된 모든 데이터 (1000개) 를 try_model로 임베딩한 input 데이터\n",
    "\n",
    "\n",
    "# 임베딩 변환 후 & npy 저장 전\n",
    "#### 데이터 생성\n",
    "batch = test_loader.dataset[:]\n",
    "batch['text'] = torch.tensor(batch['text']).to(device)\n",
    "\n",
    "YS = torch.tensor(batch['label']).to(device) \n",
    "XS = try_model['ebd'](batch)\n",
    "\n",
    "\n",
    "#### model 평가\n",
    "out_XS = try_model['clf'](XS, YS=None)\n",
    "\n",
    "output = out_XS.view(-1, args.n_classes)  # new shape: [32*35, 7]\n",
    "target = YS.view(-1)\n",
    "\n",
    "acc = BASE.compute_acc(output, target)\n",
    "f1 = BASE.compute_f1(output, target)\n",
    "mcc = BASE.compute_mcc(output, target)\n",
    "micro_f1_noneutral = BASE.compute_f1_micro_noneutral(output, target, labels=args['labels'])\n",
    "\n",
    "acc, f1, mcc, micro_f1_noneutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007719993591308594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9634857177734375,\n",
       " 0.9612830286849585,\n",
       " 0.4899312517676465,\n",
       " 0.4828393135725429)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임베딩 변환 후 & npy 저장 및 다시 load\n",
    "\n",
    "test_np_data = np.load('/home/hyuns6100/Mental-Heatlh-Care/onnx/emotion_test_data.npy')\n",
    "test_np_label = np.load('/home/hyuns6100/Mental-Heatlh-Care/onnx/emotion_test_label.npy')\n",
    "\n",
    "test = torch.tensor(test_np_data).to(device)\n",
    "label = torch.tensor(test_np_label).to(device)\n",
    "\n",
    "#### model 평가\n",
    "import time\n",
    "s = time.time()\n",
    "out_XS = try_model['clf'](test, YS=None)\n",
    "print(time.time() - s)\n",
    "\n",
    "\n",
    "output = out_XS.view(-1, args.n_classes)  # new shape: [32*35, 7]\n",
    "#label = YS.view(-1)\n",
    "label = label.view(-1)\n",
    "\n",
    "acc = BASE.compute_acc(output, label)\n",
    "f1 = BASE.compute_f1(output, label)\n",
    "mcc = BASE.compute_mcc(output, label)\n",
    "micro_f1_noneutral = BASE.compute_f1_micro_noneutral(output, label, labels=args['labels'])\n",
    "\n",
    "acc, f1, mcc, micro_f1_noneutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onnx -> pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnx\n",
    "\n",
    "# model = onnx.load(\"/home/hyuns6100/Mental-Heatlh-Care/onnx/emotion_calib_1.onxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_LazyBatchNorm' from 'torch.nn.modules.batchnorm' (/home/hyuns6100/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/onnx2pytorch/operations/batchnorm.py:7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatchnorm\u001b[39;00m \u001b[39mimport\u001b[39;00m _LazyNormBase\n\u001b[1;32m      9\u001b[0m     \u001b[39mclass\u001b[39;00m \u001b[39m_LazyBatchNorm\u001b[39;00m(_LazyNormBase, _BatchNorm):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_LazyNormBase' from 'torch.nn.modules.batchnorm' (/home/hyuns6100/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/hyuns6100/Mental-Heatlh-Care/emotion_clf.ipynb 셀 86\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224341552d44534c227d/home/hyuns6100/Mental-Heatlh-Care/emotion_clf.ipynb#Y203sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnx\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224341552d44534c227d/home/hyuns6100/Mental-Heatlh-Care/emotion_clf.ipynb#Y203sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnx2pytorch\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvertModel\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224341552d44534c227d/home/hyuns6100/Mental-Heatlh-Care/emotion_clf.ipynb#Y203sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/onnx2pytorch/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconvert\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvertModel\n\u001b[1;32m      3\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.4.1\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/onnx2pytorch/convert/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvertModel\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlayer\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/onnx2pytorch/convert/model.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjit\u001b[39;00m \u001b[39mimport\u001b[39;00m TracerWarning\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear\u001b[39;00m \u001b[39mimport\u001b[39;00m Identity\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnx2pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     COMPOSITE_LAYERS,\n\u001b[1;32m     16\u001b[0m     MULTIOUTPUT_LAYERS,\n\u001b[1;32m     17\u001b[0m     STANDARD_LAYERS,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnx2pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconvert\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdebug\u001b[39;00m \u001b[39mimport\u001b[39;00m debug_model_conversion\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnx2pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconvert\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperations\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     convert_operations,\n\u001b[1;32m     22\u001b[0m     get_buffer_name,\n\u001b[1;32m     23\u001b[0m     get_init_parameter,\n\u001b[1;32m     24\u001b[0m     Loop,\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/onnx2pytorch/constants.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconv\u001b[39;00m \u001b[39mimport\u001b[39;00m _ConvNd\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpooling\u001b[39;00m \u001b[39mimport\u001b[39;00m _MaxPoolNd\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnx2pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperations\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     BatchNormWrapper,\n\u001b[1;32m      6\u001b[0m     InstanceNormWrapper,\n\u001b[1;32m      7\u001b[0m     Loop,\n\u001b[1;32m      8\u001b[0m     LSTMWrapper,\n\u001b[1;32m      9\u001b[0m     Split,\n\u001b[1;32m     10\u001b[0m     TopK,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m COMPOSITE_LAYERS \u001b[39m=\u001b[39m (nn\u001b[39m.\u001b[39mSequential,)\n\u001b[1;32m     15\u001b[0m MULTIOUTPUT_LAYERS \u001b[39m=\u001b[39m (_MaxPoolNd, Loop, LSTMWrapper, Split, TopK)\n",
      "File \u001b[0;32m~/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/onnx2pytorch/operations/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madd\u001b[39;00m \u001b[39mimport\u001b[39;00m Add\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbatchnorm\u001b[39;00m \u001b[39mimport\u001b[39;00m BatchNormWrapper\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbitshift\u001b[39;00m \u001b[39mimport\u001b[39;00m BitShift\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcast\u001b[39;00m \u001b[39mimport\u001b[39;00m Cast\n",
      "File \u001b[0;32m~/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/onnx2pytorch/operations/batchnorm.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m         cls_to_become \u001b[39m=\u001b[39m _BatchNorm\n\u001b[1;32m     14\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[39m# for torch < 1.10.0\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatchnorm\u001b[39;00m \u001b[39mimport\u001b[39;00m _LazyBatchNorm\n\u001b[1;32m     19\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLazyBatchNormUnsafe\u001b[39;00m(_LazyBatchNorm):\n\u001b[1;32m     20\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, spatial\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_LazyBatchNorm' from 'torch.nn.modules.batchnorm' (/home/hyuns6100/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py)"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "import torch\n",
    "# import time\n",
    "\n",
    "# # Load the ONNX model\n",
    "# onnx_model = onnx.load(\"/home/hyuns6100/Mental-Heatlh-Care/onnx/emotion_calib_1.onxx\")\n",
    "\n",
    "# # Convert to PyTorch model\n",
    "# pytorch_model = ConvertModel(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_np_data = np.load('/home/hyuns6100/Mental-Heatlh-Care/onnx/emotion_test_data.npy')\n",
    "test_np_label = np.load('/home/hyuns6100/Mental-Heatlh-Care/onnx/emotion_test_label.npy')\n",
    "\n",
    "# test = torch.tensor(test_np_data).to(device)\n",
    "# label = torch.tensor(test_np_label).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.16.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers (from onnxruntime)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/hyuns6100/anaconda3/envs/MentalHealth/lib/python3.8/site-packages (from onnxruntime) (1.24.3)\n",
      "Requirement already satisfied: packaging in /home/hyuns6100/anaconda3/envs/MentalHealth/lib/python3.8/site-packages (from onnxruntime) (23.1)\n",
      "Requirement already satisfied: protobuf in /home/hyuns6100/anaconda3/envs/MentalHealth/lib/python3.8/site-packages (from onnxruntime) (3.20.3)\n",
      "Requirement already satisfied: sympy in /home/hyuns6100/anaconda3/envs/MentalHealth/lib/python3.8/site-packages (from onnxruntime) (1.12)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /home/hyuns6100/anaconda3/envs/MentalHealth/lib/python3.8/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Downloading onnxruntime-1.16.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Installing collected packages: flatbuffers, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-23.5.26 humanfriendly-10.0 onnxruntime-1.16.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyuns6100/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'TensorrtExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "/home/hyuns6100/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import onnx, onnxruntime\n",
    "\n",
    "onnx_path = \"/home/hyuns6100/Mental-Heatlh-Care/onnx/emo_clf_onnx.onxx\"\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "providers = [\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\"]\n",
    "session = onnxruntime.InferenceSession(onnx_path, providers=providers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: input.1 Got: 2 Expected: 3 Please fix either the inputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/hyuns6100/Mental-Heatlh-Care/emotion_clf.ipynb 셀 90\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224341552d44534c227d/home/hyuns6100/Mental-Heatlh-Care/emotion_clf.ipynb#Y354sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m td \u001b[39min\u001b[39;00m test_np_data:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224341552d44534c227d/home/hyuns6100/Mental-Heatlh-Care/emotion_clf.ipynb#Y354sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     outputs \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrun(\u001b[39mNone\u001b[39;49;00m, {\u001b[39m\"\u001b[39;49m\u001b[39minput.1\u001b[39;49m\u001b[39m\"\u001b[39;49m: td})\n",
      "File \u001b[0;32m~/anaconda3/envs/MentalHealth/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[39m=\u001b[39m [output\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sess\u001b[39m.\u001b[39;49mrun(output_names, input_feed, run_options)\n\u001b[1;32m    221\u001b[0m \u001b[39mexcept\u001b[39;00m C\u001b[39m.\u001b[39mEPFail \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: input.1 Got: 2 Expected: 3 Please fix either the inputs or the model."
     ]
    }
   ],
   "source": [
    "for td in test_np_data:\n",
    "    outputs = session.run(None, {\"input.1\": td})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

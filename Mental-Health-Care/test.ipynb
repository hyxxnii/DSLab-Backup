{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import easydict\n",
    "import pickle\n",
    "\n",
    "# 데이터/파라미터 경로 설정 필요\n",
    "dpath = \"\"\n",
    "\n",
    "with open(dpath + '/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({    \n",
    "    'cnn_num_filters': 100,\n",
    "    'cnn_filter_sizes': [3,4,5],\n",
    "    'context_size': 35,\n",
    "    'maxtokens': 30,\n",
    "    'mlp_hidden': [300,300],\n",
    "    'dropout': 0.1,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WORDEBD(nn.Module):\n",
    "    '''\n",
    "        An embedding layer that maps the token id into its corresponding word\n",
    "        embeddings. The word embeddings are kept as fixed once initialized.\n",
    "    '''\n",
    "    def __init__(self, vocab, finetune_ebd):#, specific_vocab_size=None):\n",
    "        super(WORDEBD, self).__init__()\n",
    "\n",
    "        self.vocab_size, self.embedding_dim = vocab.vectors.size()\n",
    "        # if specific_vocab_size != None: self.vocab_size = specific_vocab_size\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "                self.vocab_size, self.embedding_dim)\n",
    "        self.embedding_layer.weight.data = vocab.vectors\n",
    "\n",
    "        self.finetune_ebd = finetune_ebd\n",
    "\n",
    "        if self.finetune_ebd:\n",
    "            self.embedding_layer.weight.requires_grad = True\n",
    "        else:\n",
    "            self.embedding_layer.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, data, weights=None):\n",
    "        '''\n",
    "            @param text: batch_size * max_text_len\n",
    "            @return output: batch_size * max_text_len * embedding_dim\n",
    "        '''\n",
    "        if (weights is None): #or (self.finetune_ebd == False):\n",
    "            return self.embedding_layer(data['text'])\n",
    "\n",
    "\n",
    "class CNNseq(nn.Module):\n",
    "    '''\n",
    "        An aggregation method that encodes every document through different\n",
    "        convolution filters (followed by max-over-time pooling).\n",
    "    '''\n",
    "    def __init__(self, ebd, args):\n",
    "        super(CNNseq, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.ebd = ebd # pre-trained FastText로 initialization된 token representation => WORDEBD => nn.Embedding layer로 매핑된 것\n",
    "\n",
    "        self.input_dim = self.ebd.embedding_dim\n",
    "\n",
    "        # Convolution\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(\n",
    "                    in_channels=self.input_dim,\n",
    "                    out_channels=args.cnn_num_filters,\n",
    "                    kernel_size=K) for K in args.cnn_filter_sizes])\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.ebd_dim = args.cnn_num_filters * len(args.cnn_filter_sizes)\n",
    "\n",
    "    def _conv_max_pool(self, x, conv_filter=None, weights=None):\n",
    "        '''\n",
    "        Compute sentence level convolution\n",
    "        Input:\n",
    "            x:      batch_size, max_doc_len, embedding_dim\n",
    "        Output:     batch_size, num_filters_total\n",
    "        '''\n",
    "        assert(len(x.size()) == 3) # [batch_size==max_sentences, max_tokens, embedding_dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # batch_size, embedding_dim, doc_len\n",
    "        x = x.contiguous()\n",
    "\n",
    "        # Apply the 1d conv. Resulting dimension is\n",
    "        # [batch_size, num_filters, doc_len-filter_size+1] * len(filter_size)\n",
    "        assert(not ((conv_filter is None) and (weights is None)))\n",
    "        if conv_filter is not None:\n",
    "            x = [conv(x) for conv in conv_filter]\n",
    "\n",
    "        # elif weights is not None:\n",
    "        #     x = [F.conv1d(x, weight=weights['convs.{}.weight'.format(i)],\n",
    "        #                 bias=weights['convs.{}.bias'.format(i)])\n",
    "        #         for i in range(len(self.args.cnn_filter_sizes))]\n",
    "\n",
    "        ## max pool over time. Resulting dimension is\n",
    "        ## [batch_size, num_filters] * len(filter_size)\n",
    "        #x = [F.max_pool1d(sub_x, sub_x.size(2)).squeeze(2) for sub_x in x]\n",
    "        \n",
    "        ## nn.MaxPool1d로 다시 생성\n",
    "        max_pooled_outputs = []\n",
    "        for sub_x in x:\n",
    "            pool_size = sub_x.size(2)\n",
    "            max_pool = nn.MaxPool1d(pool_size) # output shape: [batch_size, num_filters, 1]\n",
    "            pooled = max_pool(sub_x).squeeze(2) # output shape: [batch_size, num_filters]\n",
    "            max_pooled_outputs.append(pooled)\n",
    "        \n",
    "        # concatenate along all filters. Resulting dimension is\n",
    "        # output: [batch_size, num_filters_total]\n",
    "        x = torch.cat(max_pooled_outputs, 1) # output shape: [batch_size, num_filters*3]\n",
    "        #x = torch.cat(x, 1)\n",
    "        x = self.relu(x) #F.relu(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, data, weights=None):\n",
    "        '''\n",
    "            @param data dictionary\n",
    "                @key text: batch_size * max_text_len\n",
    "            @param weights placeholder used for maml\n",
    "\n",
    "            @return output: batch_size * embedding_dim\n",
    "        '''\n",
    "\n",
    "        device = data['text'].device\n",
    "        \n",
    "        # Apply the word embedding, result:  batch_size, doc_len, embedding_dim\n",
    "        \n",
    "        ebd = self.ebd(data, weights) # ouptut: [batch_size, max_sentences, max_tokens, embedding_dim]\n",
    "\n",
    "        # apply 1d conv + max pool, result:  batch_size, num_filters_total        \n",
    "        ref = tuple(data['text'].size())\n",
    "        shape = (ref[0], ref[1], ( len(self.args.cnn_filter_sizes) * self.args.cnn_num_filters))\n",
    "        output = torch.randn(shape).to(device)\n",
    "        \n",
    "        if weights is None:\n",
    "            for i in range(ebd.size(0)): # 각 배치에 대해\n",
    "                out = self._conv_max_pool(ebd[i], conv_filter=self.convs) # 각 문장에 대해 처리 => (35, 300)\n",
    "                output[i] = out\n",
    "        \n",
    "        else:\n",
    "            for i in range(ebd.size(0)):\n",
    "                out = self._conv_max_pool(ebd[i], weights=weights)\n",
    "                output[i] = out\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPseq(nn.Module):\n",
    "    def __init__(self, ebd_dim, args, top_layer=None):\n",
    "        super(MLPseq, self).__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.ebd_dim = ebd_dim\n",
    "\n",
    "        self.mlp = self._init_mlp(ebd_dim, self.args.mlp_hidden, self.args.dropout)\n",
    "        self.out = self.get_top_layer(self.args, self.args.n_classes)\n",
    "        #self.top_layer = top_layer\n",
    "        self.dropout = nn.Dropout(self.args.dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_top_layer(args, n_classes):\n",
    "        '''\n",
    "            Creates final layer of desired type\n",
    "            @return final classification layer\n",
    "        '''\n",
    "        return nn.Linear(args.mlp_hidden[-1], n_classes)\n",
    "\n",
    "        \n",
    "    def _init_mlp(self, in_d, hidden_ds, drop_rate):\n",
    "        modules = []\n",
    "\n",
    "        for d in hidden_ds[:-1]:\n",
    "            modules.extend([\n",
    "                nn.Dropout(drop_rate),\n",
    "                nn.Linear(in_d, d),\n",
    "                nn.ReLU()])\n",
    "            in_d = d\n",
    "\n",
    "        modules.extend([\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(in_d, hidden_ds[-1])])\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, XS, YS=None, XQ=None, YQ=None, weights=None, return_preds=False):\n",
    "        '''\n",
    "            if y is specified, return loss and accuracy\n",
    "            otherwise, return the transformed x\n",
    "\n",
    "            @param: XS: batch_size * input_dim\n",
    "            @param: YS: batch_size (optional)\n",
    "\n",
    "            @return: XS: batch_size * output_dim\n",
    "        '''\n",
    "\n",
    "        XS = self.mlp(XS)\n",
    "        XS = self.out(XS) # output: [batch, max_sentence, n_class]\n",
    "        return XS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 모델 load 및 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 state path\n",
    "ebd_model_path = dpath + '/best.ebd'\n",
    "clf_model_path = dpath + '/best.clf'\n",
    "\n",
    "ebd_model_params = torch.load(ebd_model_path, map_location=device)\n",
    "clf_model_params = torch.load(clf_model_path, map_location=device)\n",
    "\n",
    "# 모델 정의 및 불러온 파라미터 설정\n",
    "try_model = {}\n",
    "wordebd = WORDEBD(vocab, finetune_ebd=False)\n",
    "ebd = CNNseq(wordebd, args).to(device)\n",
    "try_model['ebd'] = ebd\n",
    "\n",
    "clf = MLPseq(try_model[\"ebd\"].ebd_dim, args).to(device)\n",
    "try_model['clf'] = clf\n",
    "\n",
    "try_model['ebd'].load_state_dict(ebd_model_params)\n",
    "try_model['clf'].load_state_dict(clf_model_params)\n",
    "\n",
    "try_model['ebd'].eval()\n",
    "try_model['clf'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터 로드\n",
    "\n",
    "test_np_data = np.load(dpath + '/emotion_test_data.npy')\n",
    "test_np_label = np.load(dpath + '/emotion_test_label.npy')\n",
    "\n",
    "test = torch.tensor(test_np_data).to(device)\n",
    "label = torch.tensor(test_np_label).to(device)\n",
    "\n",
    "# 모델 inference\n",
    "out_XS = try_model['clf'](test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MentalHealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
